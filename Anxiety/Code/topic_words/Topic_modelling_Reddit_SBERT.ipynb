{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smriti/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from topic_model_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=17\n",
    "#Load Dataset\n",
    "df=pd.read_csv('/home/smriti/Smriti/MITACS/Anxiety/Data/CSV/Reddit/rAnxiety16.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#getting rid of deleted values\n",
    "df['Text']=df['Text'].replace('[deleted]','')\n",
    "#Combining title and text\n",
    "df[\"Post\"] = df[\"Title\"] + df[\"Text\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['Title', 'Text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>2016-01-06 20:10:45 EST-0500</td>\n",
       "      <td>You know you have social anxiety when you say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-01 02:22:08 EST-0500</td>\n",
       "      <td>15 TED talks on anxiety, fear, and mental well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>2016-01-02 02:24:49 EST-0500</td>\n",
       "      <td>Anyone else strangely calm in actual emergenci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>2016-01-03 07:58:52 EST-0500</td>\n",
       "      <td>Fuck social anxiety, this is going to be the y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>2016-01-02 19:38:52 EST-0500</td>\n",
       "      <td>DAE feel like they are constantly feeling watc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Comments                   Date Posted  \\\n",
       "0                  53  2016-01-06 20:10:45 EST-0500   \n",
       "1                   4  2016-01-01 02:22:08 EST-0500   \n",
       "2                  89  2016-01-02 02:24:49 EST-0500   \n",
       "3                  16  2016-01-03 07:58:52 EST-0500   \n",
       "4                  36  2016-01-02 19:38:52 EST-0500   \n",
       "\n",
       "                                                Post  \n",
       "0  You know you have social anxiety when you say ...  \n",
       "1  15 TED talks on anxiety, fear, and mental well...  \n",
       "2  Anyone else strangely calm in actual emergenci...  \n",
       "3  Fuck social anxiety, this is going to be the y...  \n",
       "4  DAE feel like they are constantly feeling watc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Post.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'you', 'have', 'social', 'anxiety', 'when', 'you', 'say', 'hope', 'did', 'ok', 'after', 'youve', 'had', 'conversation', 'with', 'someone']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'know', 'you', 'have', 'social', 'anxiety', 'when', 'you', 'say', 'hope', 'did', 'ok', 'after', 'youve', 'had', 'conversation', 'with', 'someone']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['know', 'social', 'anxiety', 'say', 'hope', 've', 'conversation']]\n",
      "There are 842 unique words in the dictionary, 842 remain after filtering out lest frequent.\n",
      "842 remain after filtering out most commonly used words based on tfidf scores.\n"
     ]
    }
   ],
   "source": [
    "#----- CHANGED ------#\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# 2. Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# 3. Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# 4. Term Document Frequency and Create a bag of words\n",
    "bow_corpus = bow(dictionary=id2word, processed_docs=texts)\n",
    "\n",
    "# 5. Calculate low_tfidf_words\n",
    "# Keep only words with tfidf ranking <= x * len(dictionary)\n",
    "x = 0.2\n",
    "total_word_count, DictDocFreq = tf_df(bow_corpus, id2word)\n",
    "sorted_TFIDF = sort_tfidf(bow_corpus, total_word_count, DictDocFreq)\n",
    "low_tfidf_words = get_low_tfidf_words(x, id2word, sorted_TFIDF)\n",
    "\n",
    "# 6. Filter out least frequently used words\n",
    "no_below = 0.01\n",
    "keep_n = 10000\n",
    "dict_least_freq_filtered = filter_least_frequent(id2word, texts, \n",
    "                                                 no_below, keep_n)\n",
    "\n",
    "# 7. Filter out most commonly used words (i.e. words with low TF-IDF score)\n",
    "dict_tfidf_filtered = filter_most_common(dict_least_freq_filtered, low_tfidf_words)\n",
    "\n",
    "# 8. Create the second bag of words - bow_corpus_TFIDFfiltered, \n",
    "# created after least frequently and most commonly used words were filtered out.\n",
    "corpus = bow(dict_tfidf_filtered, texts)\n",
    "\n",
    "# View\n",
    "[[(dict_tfidf_filtered[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dict_tfidf_filtered,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.052*\"go\" + 0.028*\"m\" + 0.023*\"day\" + 0.022*\"life\" + 0.018*\"think\" + '\n",
      "  '0.018*\"make\" + 0.016*\"year\" + 0.015*\"time\" + 0.014*\"thing\" + 0.012*\"get\"'),\n",
      " (1,\n",
      "  '0.098*\"happy\" + 0.073*\"open\" + 0.065*\"movie\" + 0.065*\"door\" + 0.059*\"love\" '\n",
      "  '+ 0.048*\"light\" + 0.046*\"name\" + 0.038*\"true\" + 0.038*\"draw\" + '\n",
      "  '0.035*\"allow\"'),\n",
      " (2,\n",
      "  '0.034*\"work\" + 0.033*\"week\" + 0.027*\"help\" + 0.027*\"take\" + 0.024*\"doctor\" '\n",
      "  '+ 0.021*\"change\" + 0.020*\"therapy\" + 0.017*\"therapist\" + 0.017*\"experience\" '\n",
      "  '+ 0.017*\"medication\"'),\n",
      " (3,\n",
      "  '0.065*\"grow\" + 0.061*\"away\" + 0.059*\"family\" + 0.058*\"man\" + '\n",
      "  '0.055*\"outside\" + 0.054*\"early\" + 0.048*\"move\" + 0.039*\"scared\" + '\n",
      "  '0.037*\"stay\" + 0.036*\"parent\"'),\n",
      " (4,\n",
      "  '0.058*\"feel\" + 0.047*\"people\" + 0.033*\"make\" + 0.031*\"know\" + 0.027*\"want\" '\n",
      "  '+ 0.026*\"even\" + 0.026*\"always\" + 0.025*\"m\" + 0.023*\"think\" + 0.022*\"talk\"'),\n",
      " (5,\n",
      "  '0.177*\"exercise\" + 0.170*\"meditation\" + 0.097*\"alcohol\" + 0.088*\"great\" + '\n",
      "  '0.065*\"intrusive_thought\" + 0.058*\"self\" + 0.046*\"important\" + 0.003*\"pm\" + '\n",
      "  '0.000*\"day\" + 0.000*\"strong\"'),\n",
      " (6,\n",
      "  '0.120*\"sleep\" + 0.096*\"hour\" + 0.076*\"night\" + 0.068*\"day\" + 0.060*\"wake\" + '\n",
      "  '0.035*\"bed\" + 0.032*\"work\" + 0.029*\"week\" + 0.023*\"car\" + '\n",
      "  '0.023*\"yesterday\"'),\n",
      " (7,\n",
      "  '0.590*\"anyone_else\" + 0.117*\"rather\" + 0.056*\"loud\" + 0.015*\"stand\" + '\n",
      "  '0.007*\"phone\" + 0.006*\"music\" + 0.000*\"mess\" + 0.000*\"noise\" + '\n",
      "  '0.000*\"remove\" + 0.000*\"animal\"'),\n",
      " (8,\n",
      "  '0.416*\"job\" + 0.111*\"work\" + 0.098*\"money\" + 0.070*\"due\" + 0.065*\"pay\" + '\n",
      "  '0.029*\"decision\" + 0.024*\"full\" + 0.019*\"interact\" + 0.018*\"interaction\" + '\n",
      "  '0.017*\"parent\"'),\n",
      " (9,\n",
      "  '0.170*\"fight\" + 0.141*\"breathe\" + 0.091*\"breath\" + 0.075*\"respond\" + '\n",
      "  '0.064*\"breathing\" + 0.059*\"destroy\" + 0.055*\"control\" + 0.054*\"catch\" + '\n",
      "  '0.048*\"lack\" + 0.038*\"race\"'),\n",
      " (10,\n",
      "  '0.210*\"thought\" + 0.128*\"laugh\" + 0.126*\"thinking\" + 0.112*\"negative\" + '\n",
      "  '0.097*\"eat\" + 0.061*\"plan\" + 0.043*\"present\" + 0.042*\"instead\" + '\n",
      "  '0.034*\"positive\" + 0.020*\"spend\"'),\n",
      " (11,\n",
      "  '0.271*\"anxious\" + 0.137*\"re\" + 0.091*\"dae\" + 0.076*\"fine\" + 0.053*\"public\" '\n",
      "  '+ 0.044*\"numb\" + 0.043*\"weekend\" + 0.034*\"voice\" + 0.024*\"event\" + '\n",
      "  '0.024*\"social_media\"'),\n",
      " (12,\n",
      "  '0.069*\"feel\" + 0.036*\"get\" + 0.033*\"help\" + 0.020*\"know\" + 0.020*\"time\" + '\n",
      "  '0.016*\"much\" + 0.016*\"really\" + 0.015*\"try\" + 0.014*\"find\" + 0.014*\"thank\"'),\n",
      " (13,\n",
      "  '0.037*\"get\" + 0.030*\"say\" + 0.028*\"friend\" + 0.025*\"tell\" + 0.022*\"see\" + '\n",
      "  '0.020*\"come\" + 0.018*\"people\" + 0.018*\"ask\" + 0.018*\"good\" + 0.016*\"first\"'),\n",
      " (14,\n",
      "  '0.198*\"fear\" + 0.142*\"pass\" + 0.118*\"heart\" + 0.064*\"pressure\" + '\n",
      "  '0.057*\"stick\" + 0.057*\"insane\" + 0.048*\"sense\" + 0.045*\"head\" + '\n",
      "  '0.040*\"aware\" + 0.029*\"slow\"'),\n",
      " (15,\n",
      "  '0.177*\"house\" + 0.099*\"stupid\" + 0.094*\"walk\" + 0.059*\"food\" + '\n",
      "  '0.057*\"whenever\" + 0.055*\"upset\" + 0.054*\"fuck\" + 0.044*\"leave\" + '\n",
      "  '0.039*\"buy\" + 0.033*\"hide\"'),\n",
      " (16,\n",
      "  '0.479*\"today\" + 0.083*\"move\" + 0.076*\"quit\" + 0.066*\"finish\" + 0.042*\"kick\" '\n",
      "  '+ 0.040*\"shitty\" + 0.034*\"terrify\" + 0.032*\"dinner\" + 0.017*\"thank\" + '\n",
      "  '0.014*\"work\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.698158400337519\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.33735637119195017\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'm', 'day', 'life', 'think', 'make', 'year', 'time', 'thing', 'get']\n",
      "['happy', 'open', 'movie', 'door', 'love', 'light', 'name', 'true', 'draw', 'allow']\n",
      "['work', 'week', 'help', 'take', 'doctor', 'change', 'therapy', 'therapist', 'experience', 'medication']\n",
      "['grow', 'away', 'family', 'man', 'outside', 'early', 'move', 'scared', 'stay', 'parent']\n",
      "['feel', 'people', 'make', 'know', 'want', 'even', 'always', 'm', 'think', 'talk']\n",
      "['exercise', 'meditation', 'alcohol', 'great', 'intrusive_thought', 'self', 'important', 'pm', 'day', 'strong']\n",
      "['sleep', 'hour', 'night', 'day', 'wake', 'bed', 'work', 'week', 'car', 'yesterday']\n",
      "['anyone_else', 'rather', 'loud', 'stand', 'phone', 'music', 'mess', 'noise', 'remove', 'animal']\n",
      "['job', 'work', 'money', 'due', 'pay', 'decision', 'full', 'interact', 'interaction', 'parent']\n",
      "['fight', 'breathe', 'breath', 'respond', 'breathing', 'destroy', 'control', 'catch', 'lack', 'race']\n",
      "['thought', 'laugh', 'thinking', 'negative', 'eat', 'plan', 'present', 'instead', 'positive', 'spend']\n",
      "['anxious', 're', 'dae', 'fine', 'public', 'numb', 'weekend', 'voice', 'event', 'social_media']\n",
      "['feel', 'get', 'help', 'know', 'time', 'much', 'really', 'try', 'find', 'thank']\n",
      "['get', 'say', 'friend', 'tell', 'see', 'come', 'people', 'ask', 'good', 'first']\n",
      "['fear', 'pass', 'heart', 'pressure', 'stick', 'insane', 'sense', 'head', 'aware', 'slow']\n",
      "['house', 'stupid', 'walk', 'food', 'whenever', 'upset', 'fuck', 'leave', 'buy', 'hide']\n",
      "['today', 'move', 'quit', 'finish', 'kick', 'shitty', 'terrify', 'dinner', 'thank', 'work']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,num_topics):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Year, Source, Topic_ID, Most_freq_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Year':[],'Source':[],'Topic_ID':[],'Most_freq_words':[]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Most_freq_words']=topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Year='2016')\n",
    "df = df.assign(Source='Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for i in range(0,num_topics):\n",
    "    ls.append(i)\n",
    "df['Topic_ID']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>[go, m, day, life, think, make, year, time, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>[happy, open, movie, door, love, light, name, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>2</td>\n",
       "      <td>[work, week, help, take, doctor, change, thera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>[grow, away, family, man, outside, early, move...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>[feel, people, make, know, want, even, always,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Source  Topic_ID                                    Most_freq_words\n",
       "0  2016  Reddit         0  [go, m, day, life, think, make, year, time, th...\n",
       "1  2016  Reddit         1  [happy, open, movie, door, love, light, name, ...\n",
       "2  2016  Reddit         2  [work, week, help, take, doctor, change, thera...\n",
       "3  2016  Reddit         3  [grow, away, family, man, outside, early, move...\n",
       "4  2016  Reddit         4  [feel, people, make, know, want, even, always,..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"topic_words_r2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
