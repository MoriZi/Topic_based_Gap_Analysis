{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smriti/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from topic_model_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=35\n",
    "#Load Dataset\n",
    "df=pd.read_csv('/home/smriti/Smriti/MITACS/Anxiety/Data/CSV/Reddit/rAnxiety20.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#getting rid of deleted values\n",
    "df['Text']=df['Text'].replace('[deleted]','')\n",
    "#Combining title and text\n",
    "df[\"Post\"] = df[\"Title\"] + df[\"Text\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['Title', 'Text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>2020-04-15 15:12:25 EDT-0400</td>\n",
       "      <td>When you write a freaking essay on here and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "      <td>2020-04-16 09:38:14 EDT-0400</td>\n",
       "      <td>I wish I knew what it was like to not have anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>2020-04-16 01:41:14 EDT-0400</td>\n",
       "      <td>Probably the most exhausting part about anxiet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161</td>\n",
       "      <td>2020-04-16 21:48:01 EDT-0400</td>\n",
       "      <td>Has anyone else ever exerienced a period where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122</td>\n",
       "      <td>2020-04-15 06:34:07 EDT-0400</td>\n",
       "      <td>I fucking love ProzacIt calms me down sooo muc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Comments                   Date Posted  \\\n",
       "0                  55  2020-04-15 15:12:25 EDT-0400   \n",
       "1                  95  2020-04-16 09:38:14 EDT-0400   \n",
       "2                  62  2020-04-16 01:41:14 EDT-0400   \n",
       "3                 161  2020-04-16 21:48:01 EDT-0400   \n",
       "4                 122  2020-04-15 06:34:07 EDT-0400   \n",
       "\n",
       "                                                Post  \n",
       "0  When you write a freaking essay on here and th...  \n",
       "1  I wish I knew what it was like to not have anx...  \n",
       "2  Probably the most exhausting part about anxiet...  \n",
       "3  Has anyone else ever exerienced a period where...  \n",
       "4  I fucking love ProzacIt calms me down sooo muc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Post.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when', 'you', 'write', 'freaking', 'essay', 'on', 'here', 'and', 'then', 'delete', 'it', 'because', 'you', 'know', 'anxiety']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'you', 'write', 'freaking', 'essay', 'on', 'here', 'and', 'then', 'delete', 'it', 'because', 'you', 'know', 'anxiety']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=eec185cf31a95f0a46c7f219f209ab0c3f4ca72ec763f13254112993804e9c4e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bdj_vf11/wheels/83/63/50/6467284e1c2d50ffcf02e6582680fd16c6375748a94f75c1a0\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.0.0\n",
      "    Uninstalling en-core-web-sm-3.0.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.0.0\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/smriti/.local/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/home/smriti/.local/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "[['write', 'freak', 'essay', 'delete', 'know', 'anxiety']]\n",
      "There are 674 unique words in the dictionary, 674 remain after filtering out lest frequent.\n",
      "674 remain after filtering out most commonly used words based on tfidf scores.\n"
     ]
    }
   ],
   "source": [
    "#----- CHANGED ------#\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "!python3 -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# 1. Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# 2. Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# 3. Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# 4. Term Document Frequency and Create a bag of words\n",
    "bow_corpus = bow(dictionary=id2word, processed_docs=texts)\n",
    "\n",
    "# 5. Calculate low_tfidf_words\n",
    "# Keep only words with tfidf ranking <= x * len(dictionary)\n",
    "x = 0.2\n",
    "total_word_count, DictDocFreq = tf_df(bow_corpus, id2word)\n",
    "sorted_TFIDF = sort_tfidf(bow_corpus, total_word_count, DictDocFreq)\n",
    "low_tfidf_words = get_low_tfidf_words(x, id2word, sorted_TFIDF)\n",
    "\n",
    "# 6. Filter out least frequently used words\n",
    "no_below = 0.01\n",
    "keep_n = 10000\n",
    "dict_least_freq_filtered = filter_least_frequent(id2word, texts, \n",
    "                                                 no_below, keep_n)\n",
    "\n",
    "# 7. Filter out most commonly used words (i.e. words with low TF-IDF score)\n",
    "dict_tfidf_filtered = filter_most_common(dict_least_freq_filtered, low_tfidf_words)\n",
    "\n",
    "# 8. Create the second bag of words - bow_corpus_TFIDFfiltered, \n",
    "# created after least frequently and most commonly used words were filtered out.\n",
    "corpus = bow(dict_tfidf_filtered, texts)\n",
    "\n",
    "# View\n",
    "[[(dict_tfidf_filtered[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dict_tfidf_filtered,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24,\n",
      "  '0.001*\"annoy\" + 0.001*\"guess\" + 0.001*\"meet\" + 0.001*\"old\" + 0.001*\"panic\" '\n",
      "  '+ 0.001*\"stand\" + 0.001*\"terrify\" + 0.001*\"treat\" + 0.001*\"reality\" + '\n",
      "  '0.001*\"anymore\"'),\n",
      " (11,\n",
      "  '0.001*\"annoy\" + 0.001*\"guess\" + 0.001*\"meet\" + 0.001*\"old\" + 0.001*\"panic\" '\n",
      "  '+ 0.001*\"stand\" + 0.001*\"terrify\" + 0.001*\"treat\" + 0.001*\"reality\" + '\n",
      "  '0.001*\"anymore\"'),\n",
      " (10,\n",
      "  '0.433*\"write\" + 0.326*\"freak\" + 0.085*\"delete\" + 0.000*\"know\" + '\n",
      "  '0.000*\"terrify\" + 0.000*\"old\" + 0.000*\"reality\" + 0.000*\"treat\" + '\n",
      "  '0.000*\"extra\" + 0.000*\"panic\"'),\n",
      " (29,\n",
      "  '0.258*\"leave\" + 0.189*\"husband\" + 0.159*\"offer\" + 0.137*\"nee\" + '\n",
      "  '0.102*\"part\" + 0.000*\"go\" + 0.000*\"treat\" + 0.000*\"stand\" + 0.000*\"reality\" '\n",
      "  '+ 0.000*\"lack\"'),\n",
      " (33,\n",
      "  '0.334*\"test\" + 0.300*\"covid\" + 0.196*\"remove\" + 0.039*\"positive\" + '\n",
      "  '0.000*\"parent\" + 0.000*\"reality\" + 0.000*\"annoy\" + 0.000*\"extra\" + '\n",
      "  '0.000*\"stand\" + 0.000*\"treat\"'),\n",
      " (18,\n",
      "  '0.231*\"close\" + 0.214*\"constant\" + 0.095*\"family\" + 0.091*\"question\" + '\n",
      "  '0.073*\"uncomfortable\" + 0.069*\"public\" + 0.057*\"nightmare\" + '\n",
      "  '0.053*\"overthinke\" + 0.027*\"past\" + 0.000*\"thing\"'),\n",
      " (19,\n",
      "  '0.279*\"normal\" + 0.172*\"focus\" + 0.154*\"breathing\" + 0.115*\"usually\" + '\n",
      "  '0.067*\"recommend\" + 0.060*\"state\" + 0.032*\"experience\" + 0.024*\"seriously\" '\n",
      "  '+ 0.018*\"awful\" + 0.006*\"anyway\"'),\n",
      " (9,\n",
      "  '0.337*\"attack\" + 0.093*\"thinking\" + 0.083*\"arm\" + 0.082*\"throat\" + '\n",
      "  '0.073*\"heart_rate\" + 0.069*\"tight\" + 0.061*\"usually\" + 0.041*\"chance\" + '\n",
      "  '0.024*\"love\" + 0.020*\"increase\"'),\n",
      " (30,\n",
      "  '0.162*\"stomach\" + 0.152*\"better\" + 0.150*\"change\" + 0.144*\"trigger\" + '\n",
      "  '0.122*\"mg\" + 0.076*\"anyway\" + 0.061*\"extreme\" + 0.055*\"certain\" + '\n",
      "  '0.009*\"issue\" + 0.005*\"fast\"'),\n",
      " (21,\n",
      "  '0.277*\"anyone_else\" + 0.245*\"school\" + 0.157*\"miss\" + 0.089*\"wait\" + '\n",
      "  '0.064*\"bathroom\" + 0.048*\"massive\" + 0.041*\"absolutely\" + 0.010*\"rather\" + '\n",
      "  '0.006*\"get\" + 0.001*\"lot\"'),\n",
      " (31,\n",
      "  '0.093*\"world\" + 0.078*\"head\" + 0.057*\"tip\" + 0.051*\"matter\" + '\n",
      "  '0.050*\"situation\" + 0.044*\"case\" + 0.041*\"see\" + 0.038*\"throw\" + '\n",
      "  '0.037*\"terrify\" + 0.034*\"thought\"'),\n",
      " (1,\n",
      "  '0.354*\"anxious\" + 0.107*\"afraid\" + 0.104*\"reason\" + 0.088*\"anyone_else\" + '\n",
      "  '0.065*\"health\" + 0.039*\"new\" + 0.033*\"cause\" + 0.027*\"message\" + '\n",
      "  '0.023*\"cycle\" + 0.020*\"problem\"'),\n",
      " (4,\n",
      "  '0.114*\"anyone\" + 0.055*\"experience\" + 0.053*\"someone\" + 0.052*\"advice\" + '\n",
      "  '0.037*\"super\" + 0.036*\"see\" + 0.034*\"nervous\" + 0.031*\"sound\" + '\n",
      "  '0.031*\"least\" + 0.031*\"shake\"'),\n",
      " (34,\n",
      "  '0.107*\"tell\" + 0.091*\"want\" + 0.072*\"say\" + 0.053*\"read\" + 0.051*\"know\" + '\n",
      "  '0.048*\"small\" + 0.047*\"scare\" + 0.046*\"scared\" + 0.036*\"ask\" + '\n",
      "  '0.033*\"hurt\"'),\n",
      " (3,\n",
      "  '0.088*\"call\" + 0.058*\"something\" + 0.058*\"constantly\" + 0.047*\"happen\" + '\n",
      "  '0.045*\"wrong\" + 0.042*\"phone\" + 0.037*\"worried\" + 0.037*\"get\" + 0.036*\"say\" '\n",
      "  '+ 0.034*\"sometimes\"'),\n",
      " (6,\n",
      "  '0.066*\"finally\" + 0.058*\"week\" + 0.056*\"put\" + 0.048*\"hour\" + 0.044*\"take\" '\n",
      "  '+ 0.039*\"day\" + 0.036*\"today\" + 0.033*\"home\" + 0.032*\"start\" + '\n",
      "  '0.031*\"panic_attack\"'),\n",
      " (12,\n",
      "  '0.225*\"not\" + 0.216*\"be\" + 0.132*\"do\" + 0.090*\"have\" + 0.039*\"s\" + '\n",
      "  '0.022*\"know\" + 0.012*\"ill\" + 0.012*\"go\" + 0.012*\"there\" + 0.009*\"try\"'),\n",
      " (17,\n",
      "  '0.065*\"day\" + 0.055*\"go\" + 0.034*\"keep\" + 0.033*\"back\" + 0.029*\"take\" + '\n",
      "  '0.025*\"wake\" + 0.024*\"away\" + 0.022*\"moment\" + 0.020*\"pass\" + 0.019*\"turn\"'),\n",
      " (28,\n",
      "  '0.096*\"help\" + 0.072*\"work\" + 0.054*\"take\" + 0.051*\"need\" + 0.046*\"try\" + '\n",
      "  '0.037*\"say\" + 0.036*\"would\" + 0.030*\"give\" + 0.023*\"get\" + 0.023*\"see\"'),\n",
      " (16,\n",
      "  '0.089*\"feel\" + 0.056*\"get\" + 0.040*\"go\" + 0.036*\"make\" + 0.034*\"time\" + '\n",
      "  '0.031*\"know\" + 0.030*\"thing\" + 0.029*\"really\" + 0.027*\"think\" + '\n",
      "  '0.024*\"bad\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.6688319699720005\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3004801663714596\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'house', 'anymore', 'old', 'panic_attack', 'panic', 'reach', 'disorder', 'ago', 'grow']\n",
      "['anxious', 'afraid', 'reason', 'anyone_else', 'health', 'new', 'cause', 'message', 'cycle', 'problem']\n",
      "['brain', 'body', 'right', 'lately', 'severe', 'guess', 'extra', 'panic_attack', 'suddenly', 'hard']\n",
      "['call', 'something', 'constantly', 'happen', 'wrong', 'phone', 'worried', 'get', 'say', 'sometimes']\n",
      "['anyone', 'experience', 'someone', 'advice', 'super', 'see', 'nervous', 'sound', 'least', 'shake']\n",
      "['talk', 'stop', 'support', 'think', 'wanna', 'hate', 'literally', 'pretty', 'nothing', 'far']\n",
      "['finally', 'week', 'put', 'hour', 'take', 'day', 'today', 'home', 'start', 'panic_attack']\n",
      "['sleep', 'eat', 'night', 'morning', 'tomorrow', 'terrible', 'vent', 'sick', 'allow', 'today']\n",
      "['post', 'high', 'side', 'nausea', 'anyone', 'pandemic', 'struggle', 'helpful', 'pill', 'graduate']\n",
      "['attack', 'thinking', 'arm', 'throat', 'heart_rate', 'tight', 'usually', 'chance', 'love', 'increase']\n",
      "['write', 'freak', 'delete', 'know', 'terrify', 'old', 'reality', 'treat', 'extra', 'panic']\n",
      "['annoy', 'guess', 'meet', 'old', 'panic', 'stand', 'terrify', 'treat', 'reality', 'anymore']\n",
      "['not', 'be', 'do', 'have', 's', 'know', 'ill', 'go', 'there', 'try']\n",
      "['someone', 'social_media', 'breath', 'kinda', 'follow', 'else', 'loss', 'clean', 'clear', 'name']\n",
      "['people', 'way', 'real', 'hate', 'hold', 'annoying', 'life', 'much', 'social', 'feel']\n",
      "['care', 'sometimes', 'worry', 'way', 'daily', 'send', 'text', 'concern', 'step', 'contact']\n",
      "['feel', 'get', 'go', 'make', 'time', 'know', 'thing', 'really', 'think', 'bad']\n",
      "['day', 'go', 'keep', 'back', 'take', 'wake', 'away', 'moment', 'pass', 'turn']\n",
      "['close', 'constant', 'family', 'question', 'uncomfortable', 'public', 'nightmare', 'overthinke', 'past', 'thing']\n",
      "['normal', 'focus', 'breathing', 'usually', 'recommend', 'state', 'experience', 'seriously', 'awful', 'anyway']\n",
      "['doctor', 'heart', 'symptom', 'drive', 'right', 'run', 'month', 'normal', 'med', 'fine']\n",
      "['anyone_else', 'school', 'miss', 'wait', 'bathroom', 'massive', 'absolutely', 'rather', 'get', 'lot']\n",
      "['therapy', 'appointment', 'depression', 'future', 'suppose', 'group', 'question', 'girlfriend', 'want', 'talk']\n",
      "['fear', 'fuck', 'die', 'weird', 'family', 'study', 'death', 'positive', 'age', 'dad']\n",
      "['annoy', 'guess', 'meet', 'old', 'panic', 'stand', 'terrify', 'treat', 'reality', 'anymore']\n",
      "['chest', 'breathe', 'lose', 'sort', 'panic', 'yesterday', 'physically', 'deep', 'begin', 'truly']\n",
      "['hope', 'panic_attack', 'day', 'deal', 'cope', 'safe', 'way', 'appreciate', 'physical', 'currently']\n",
      "['job', 'want', 'mental_health', 'due', 'money', 'guilty', 'news', 'quite', 'free', 'finish']\n",
      "['help', 'work', 'take', 'need', 'try', 'say', 'would', 'give', 'get', 'see']\n",
      "['leave', 'husband', 'offer', 'nee', 'part', 'go', 'treat', 'stand', 'reality', 'lack']\n",
      "['stomach', 'better', 'change', 'trigger', 'mg', 'anyway', 'extreme', 'certain', 'issue', 'fast']\n",
      "['world', 'head', 'tip', 'matter', 'situation', 'case', 'see', 'throw', 'terrify', 'thought']\n",
      "['happen', 'realize', 'many', 'kind', 'something', 'relate', 'share', 'story', 'game', 'edit']\n",
      "['test', 'covid', 'remove', 'positive', 'parent', 'reality', 'annoy', 'extra', 'stand', 'treat']\n",
      "['tell', 'want', 'say', 'read', 'know', 'small', 'scare', 'scared', 'ask', 'hurt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,num_topics):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Year, Source, Topic_ID, Most_freq_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Year':[],'Source':[],'Topic_ID':[],'Most_freq_words':[]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Most_freq_words']=topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Year='2020')\n",
    "df = df.assign(Source='Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for i in range(0,num_topics):\n",
    "    ls.append(i)\n",
    "df['Topic_ID']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>[year, house, anymore, old, panic_attack, pani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>[anxious, afraid, reason, anyone_else, health,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>2</td>\n",
       "      <td>[brain, body, right, lately, severe, guess, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>[call, something, constantly, happen, wrong, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>[anyone, experience, someone, advice, super, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Source  Topic_ID                                    Most_freq_words\n",
       "0  2020  Reddit         0  [year, house, anymore, old, panic_attack, pani...\n",
       "1  2020  Reddit         1  [anxious, afraid, reason, anyone_else, health,...\n",
       "2  2020  Reddit         2  [brain, body, right, lately, severe, guess, ex...\n",
       "3  2020  Reddit         3  [call, something, constantly, happen, wrong, p...\n",
       "4  2020  Reddit         4  [anyone, experience, someone, advice, super, s..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"topic_words_r2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
