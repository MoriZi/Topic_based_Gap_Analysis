{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smriti/.local/lib/python3.5/site-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "df=pd.read_csv('/home/smriti/Desktop/NLP/MITACS/Anxiety/Data/CSV/Reddit/rAnxiety13.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#getting rid of deleted values\n",
    "df['Text']=df['Text'].replace('[deleted]','')\n",
    "#Combining title and text\n",
    "df[\"Post\"] = df[\"Title\"] + df[\"Text\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['Title', 'Text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>2013-05-03 16:31:35 EDT-0400</td>\n",
       "      <td>I think this adequately describes what I go th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>2013-05-02 19:41:23 EDT-0400</td>\n",
       "      <td>My sister told me she saw this and thought of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>2013-05-01 19:18:32 EDT-0400</td>\n",
       "      <td>Just spent an hour in a coffee shop on my own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>2013-05-04 15:53:25 EDT-0400</td>\n",
       "      <td>Caution to anyone planning to watch Iron Man 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>2013-05-02 17:53:07 EDT-0400</td>\n",
       "      <td>DAE have this happen to them?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Comments                   Date Posted  \\\n",
       "0                  38  2013-05-03 16:31:35 EDT-0400   \n",
       "1                  17  2013-05-02 19:41:23 EDT-0400   \n",
       "2                  32  2013-05-01 19:18:32 EDT-0400   \n",
       "3                  60  2013-05-04 15:53:25 EDT-0400   \n",
       "4                  31  2013-05-02 17:53:07 EDT-0400   \n",
       "\n",
       "                                                Post  \n",
       "0  I think this adequately describes what I go th...  \n",
       "1  My sister told me she saw this and thought of ...  \n",
       "2  Just spent an hour in a coffee shop on my own ...  \n",
       "3     Caution to anyone planning to watch Iron Man 3  \n",
       "4                      DAE have this happen to them?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Post.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['think', 'this', 'adequately', 'describes', 'what', 'go', 'through', 'every', 'single', 'day']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'this', 'adequately', 'describes', 'what', 'go', 'through', 'every_single', 'day']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['think', 'adequately', 'describe', 'go', 'day']]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=62,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(61,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (17,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (46,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (48,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (58,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (44,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (11,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (2,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (40,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (22,\n",
      "  '0.000*\"genre\" + 0.000*\"flare\" + 0.000*\"upside\" + 0.000*\"thriller\" + '\n",
      "  '0.000*\"summary\" + 0.000*\"stigmata\" + 0.000*\"select\" + 0.000*\"missing\" + '\n",
      "  '0.000*\"hardest\" + 0.000*\"hormone\"'),\n",
      " (9,\n",
      "  '0.105*\"whenever\" + 0.094*\"word\" + 0.064*\"negative\" + 0.061*\"quickly\" + '\n",
      "  '0.047*\"man\" + 0.043*\"period\" + 0.039*\"cognitive\" + 0.038*\"rid\" + '\n",
      "  '0.035*\"improvement\" + 0.031*\"barely\"'),\n",
      " (10,\n",
      "  '0.117*\"class\" + 0.100*\"symptom\" + 0.087*\"room\" + 0.078*\"girl\" + '\n",
      "  '0.062*\"kill\" + 0.055*\"forget\" + 0.041*\"serious\" + 0.039*\"visit\" + '\n",
      "  '0.035*\"cold\" + 0.032*\"space\"'),\n",
      " (38,\n",
      "  '0.100*\"will\" + 0.093*\"relationship\" + 0.079*\"guess\" + 0.064*\"break\" + '\n",
      "  '0.061*\"friend\" + 0.059*\"constant\" + 0.054*\"boyfriend\" + 0.037*\"kind\" + '\n",
      "  '0.035*\"thinking\" + 0.031*\"together\"'),\n",
      " (5,\n",
      "  '0.125*\"little\" + 0.098*\"do\" + 0.056*\"feeling\" + 0.046*\"level\" + '\n",
      "  '0.036*\"state\" + 0.036*\"set\" + 0.027*\"entire\" + 0.027*\"negative\" + '\n",
      "  '0.026*\"fast\" + 0.024*\"suggestion\"'),\n",
      " (53,\n",
      "  '0.095*\"thank\" + 0.084*\"love\" + 0.058*\"away\" + 0.043*\"disorder\" + '\n",
      "  '0.042*\"see\" + 0.042*\"amazing\" + 0.032*\"especially\" + 0.029*\"world\" + '\n",
      "  '0.027*\"result\" + 0.026*\"past\"'),\n",
      " (24,\n",
      "  '0.103*\"post\" + 0.071*\"depression\" + 0.060*\"realize\" + 0.052*\"therapist\" + '\n",
      "  '0.051*\"learn\" + 0.045*\"recently\" + 0.039*\"other\" + 0.033*\"relate\" + '\n",
      "  '0.029*\"little\" + 0.028*\"subreddit\"'),\n",
      " (50,\n",
      "  '0.049*\"would\" + 0.035*\"could\" + 0.031*\"hour\" + 0.028*\"therapy\" + '\n",
      "  '0.024*\"issue\" + 0.024*\"next\" + 0.022*\"cry\" + 0.021*\"ask\" + 0.021*\"avoid\" + '\n",
      "  '0.019*\"night\"'),\n",
      " (57,\n",
      "  '0.137*\"be\" + 0.048*\"go\" + 0.040*\"think\" + 0.034*\"say\" + 0.027*\"tell\" + '\n",
      "  '0.027*\"feel\" + 0.023*\"s\" + 0.019*\"happen\" + 0.017*\"even\" + 0.017*\"know\"'),\n",
      " (7,\n",
      "  '0.038*\"people\" + 0.028*\"help\" + 0.026*\"make\" + 0.023*\"life\" + 0.022*\"would\" '\n",
      "  '+ 0.021*\"want\" + 0.021*\"find\" + 0.021*\"try\" + 0.020*\"thing\" + '\n",
      "  '0.019*\"really\"'),\n",
      " (15,\n",
      "  '0.070*\"anxiety\" + 0.051*\"feel\" + 0.044*\"get\" + 0.030*\"go\" + 0.026*\"have\" + '\n",
      "  '0.025*\"time\" + 0.022*\"year\" + 0.022*\"day\" + 0.021*\"take\" + 0.018*\"start\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -20.06869005650696\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3592376790249025\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 62\n",
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['ahead', 'hardest', 'butt', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hormone']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['kind', 'big', 'hit', 'tired', 'wake', 'awesome', 'tonight', 'screw', 'working', 'floor']\n",
      "['testing', 'genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest']\n",
      "['little', 'do', 'feeling', 'level', 'state', 'set', 'entire', 'negative', 'fast', 'suggestion']\n",
      "['laugh', 'shirt', 'stigmata', 'summary', 'select', 'hardest', 'dominance', 'upside', 'butt', 'thriller']\n",
      "['people', 'help', 'make', 'life', 'would', 'want', 'find', 'try', 'thing', 'really']\n",
      "['example', 'quite', 'bus', 'asshole', 'bit', 'least', 'less', 'instead', 'situation', 'often']\n",
      "['whenever', 'word', 'negative', 'quickly', 'man', 'period', 'cognitive', 'rid', 'improvement', 'barely']\n",
      "['class', 'symptom', 'room', 'girl', 'kill', 'forget', 'serious', 'visit', 'cold', 'space']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['phone', 'pm', 'alarm', 'set', 'upside', 'summary', 'select', 'thriller', 'hormone', 'dominance']\n",
      "['funny', 'article', 'regard', 'smart', 'hopeful', 'survive', 'suffering', 'condescending', 'testosterone', 'dominance']\n",
      "['create', 'link', 'research', 'information', 'survey', 'effort', 'intense', 'community', 'greatly', 'process']\n",
      "['anxiety', 'feel', 'get', 'go', 'have', 'time', 'year', 'day', 'take', 'start']\n",
      "['far', 'common', 'mine', 'overwhelming', 'concern', 'mindfulness', 'usual', 'mindful', 'activity', 'driving']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['send', 'generalize', 'meal', 'stigmata', 'upside', 'select', 'hardest', 'summary', 'dominance', 'thriller']\n",
      "['insurance', 'victory', 'cover', 'private', 'originally', 'best', 'file', 'stigmata', 'hardest', 'select']\n",
      "['form', 'agree', 'potential', 'system', 'generally', 'chronic', 'circle', 'former', 'online', 'brutal']\n",
      "['nee', 'food', 'describe', 'safe', 'awful', 'mouth', 'pack', 'welcome', 'storm', 'build']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['message', 'waste', 'late', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest']\n",
      "['post', 'depression', 'realize', 'therapist', 'learn', 'recently', 'other', 'relate', 'little', 'subreddit']\n",
      "['job', 'work', 'money', 'courage', 'country', 'application', 'full', 'hire', 'place', 'submit']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['sound', 'drink', 'grow', 'complete', 'alcohol', 'freak', 'minute', 'town', 'expensive', 'business']\n",
      "['white', 'butt', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['worry', 'here', 'repeat', 'member', 'rather', 'happen', 'recent', 'brief', 'embarrassment', 'comedy']\n",
      "['uncertainty', 'genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest']\n",
      "['healthy', 'perspective', 'issue', 'ball', 'meditate', 'guilt', 'carry', 'teenager', 'clear', 'unhappy']\n",
      "['judge', 'massive', 'red', 'sum', 'sweating', 'pic', 'sometimes', 'help', 'panic_attack', 'feel']\n",
      "['run', 'medicine', 'several', 'tree', 'chair', 'dominance', 'summary', 'select', 'thriller', 'genre']\n",
      "['proud', 'coffee', 'shop', 'mild', 'guy', 'wallet', 'may', 'straight', 'summary', 'dominance']\n",
      "['weekend', 'final', 'customer', 'cashier', 'double', 'intimidate', 'child', 'genre', 'missing', 'butt']\n",
      "['fear', 'severe', 'date', 'simple', 'leg', 'quit', 'grade', 'smoke', 'sufferer', 'failure']\n",
      "['will', 'relationship', 'guess', 'break', 'friend', 'constant', 'boyfriend', 'kind', 'thinking', 'together']\n",
      "['dread', 'holiday', 'season', 'stigmata', 'missing', 'select', 'summary', 'thriller', 'butt', 'upside']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['gym', 'easy', 'hardest', 'butt', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing']\n",
      "['dad', 'glad', 'shock', 'straight', 'second', 'choose', 'background', 'center', 'truck', 'minor']\n",
      "['topic', 'finally', 'med', 'speak', 'get', 'stigmata', 'thriller', 'summary', 'upside', 'hardest']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['like', 'butt', 'agent', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'genre']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['body', 'weight', 'disorder', 'add', 'general', 'top', 'blanket', 'pressure', 'recommend', 'cell']\n",
      "['would', 'could', 'hour', 'therapy', 'issue', 'next', 'cry', 'ask', 'avoid', 'night']\n",
      "['scared', 'motivation', 'lack', 'dominance', 'pose', 'upside', 'summary', 'hormone', 'testosterone', 'select']\n",
      "['drive', 'buy', 'specifically', 'skip', 'woman', 'instructor', 'shopping', 'place', 'advice', 'hormone']\n",
      "['thank', 'love', 'away', 'disorder', 'see', 'amazing', 'especially', 'world', 'result', 'past']\n",
      "['present', 'admit', 'summary', 'select', 'stigmata', 'upside', 'thriller', 'dominance', 'hardest', 'hormone']\n",
      "['show', 'focus', 'notice', 'com', 'trouble', 'rather', 'fine', 'watch', 'perfect', 'mood']\n",
      "['rant', 'overcome', 'lately', 'draw', 'sad', 'silly', 'paxil', 'social', 'comic_strip', 'officially']\n",
      "['be', 'go', 'think', 'say', 'tell', 'feel', 's', 'happen', 'even', 'know']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n",
      "['feeling', 'school', 'second', 'fine', 'cope', 'horrible', 'grab', 'jump', 'disease', 'cancer']\n",
      "['part', 'emotion', 'meditation', 'cut', 'exercise', 'increase', 'technique', 'http_www', 'longer', 'breath']\n",
      "['genre', 'flare', 'upside', 'thriller', 'summary', 'stigmata', 'select', 'missing', 'hardest', 'hormone']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,62):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most_freq_words</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Most_freq_words, Source, Topic_ID, Year]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Year':[],'Source':[],'Topic_ID':[],'Most_freq_words':[]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Most_freq_words']=topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Year='2013')\n",
    "df = df.assign(Source='Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for i in range(0,62):\n",
    "    ls.append(i)\n",
    "df['Topic_ID']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most_freq_words</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[genre, flare, upside, thriller, summary, stig...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ahead, hardest, butt, upside, thriller, summa...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[genre, flare, upside, thriller, summary, stig...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[kind, big, hit, tired, wake, awesome, tonight...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[testing, genre, flare, upside, thriller, summ...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Most_freq_words  Source  Topic_ID  Year\n",
       "0  [genre, flare, upside, thriller, summary, stig...  Reddit         0  2013\n",
       "1  [ahead, hardest, butt, upside, thriller, summa...  Reddit         1  2013\n",
       "2  [genre, flare, upside, thriller, summary, stig...  Reddit         2  2013\n",
       "3  [kind, big, hit, tired, wake, awesome, tonight...  Reddit         3  2013\n",
       "4  [testing, genre, flare, upside, thriller, summ...  Reddit         4  2013"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"topic_words_r2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
