{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtained Perplexity: -7.07, Coherence: 0.49, Best Number of Topics= 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smriti/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from topic_model_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=26\n",
    "year='2019'\n",
    "#Load Dataset\n",
    "df=pd.read_csv('/home/smriti/Smriti/MITACS/Anxiety/Data/CSV/Medium/medium_text_2019.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#Combining title and text\n",
    "df[\"Post\"] = df[\"title\"] + df[\"text\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['title', 'text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>My Year Without BoozeDrinking was my routine, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>Reflections From the Bathroom FloorAs I lay in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>OCD Medications Work (But You’ll Have to Try a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>Anxiety, a fight of two loversAnxiety is the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>The Old Torture Chamber Of The HeartTo dismant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                               Post\n",
       "0  2019  My Year Without BoozeDrinking was my routine, ...\n",
       "1  2019  Reflections From the Bathroom FloorAs I lay in...\n",
       "2  2019  OCD Medications Work (But You’ll Have to Try a...\n",
       "3  2019  Anxiety, a fight of two loversAnxiety is the i...\n",
       "4  2019  The Old Torture Chamber Of The HeartTo dismant..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Post.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'year', 'without', 'boozedrinking', 'was', 'my', 'routine', 'booze', 'was', 'my', 'pattern', 'red', 'wine', 'while', 'cooked', 'dinner', 'whiskey', 'while', 'got', 'ready', 'to', 'go', 'out', 'more', 'wine', 'for', 'an', 'afternoon', 'in', 'the', 'park', 'every', 'occasion', 'revolved', 'around', 'drinking', 'felt', 'fatigued', 'around', 'the', 'holidays', 'many', 'peers', 'started', 'talking', 'about', 'doing', 'sober', 'january', 'the', 'first', 'thought', 'in', 'my', 'head', 'was', 'wow', 'that', 'sounds', 'really', 'hard', 'that', 'thought', 'led', 'to', 'the', 'realization', 'that', 'if', 'it', 'seemed', 'hard', 'maybe', 'it', 'was', 'necessary', 'this', 'time', 'last', 'year', 'decided', 'to', 'attempt', 'sober', 'january', 'as', 'january', 'started', 'yoga', 'classes', 'replaced', 'the', 'hours', 'would', 'have', 'spent', 'drinking', 'with', 'friends', 'spent', 'my', 'th', 'birthday', 'on', 'saturday', 'january', 'th', 'organizing', 'my', 'kitchen', 'and', 'listening', 'to', 'podcasts', 'felt', 'myself', 'drawn', 'to', 'more', 'philosophical', 'content', 'such', 'as', 'on', 'being', 'by', 'krista', 'tippett', 'and', 'couldn', 'get', 'enough', 'social', 'justice', 'news', 'from', 'pod', 'save', 'the', 'people', 'by', 'deray', 'mckesson', 'read', 'books', 'even', 'more', 'voraciously', 'than', 'before', 'three', 'favorites', 'from', 'the', 'start', 'of', 'were', 'sing', 'unburied', 'sing', 'by', 'jesmyn', 'ward', 'red', 'clocks', 'by', 'leni', 'zumas', 'and', 'the', 'immortalists', 'by', 'chloe', 'benjamin', 'you', 'can', 'see', 'my', 'complete', 'reading', 'list', 'from', 'books', 'total', 'on', 'my', 'goodreads', 'account', 'rested', 'more', 'isolated', 'myself', 'and', 'kept', 'myself', 'busy', 'so', 'wouldn', 'be', 'tempted', 'to', 'drink', 'after', 'three', 'weeks', 'with', 'no', 'alcohol', 'about', 'percent', 'of', 'my', 'anxiety', 'and', 'depression', 'symptoms', 'went', 'away', 'everyday', 'used', 'to', 'include', 'erratic', 'mood', 'swings', 'would', 'feel', 'elated', 'and', 'edgy', 'for', 'few', 'hours', 'only', 'to', 'crash', 'down', 'to', 'feeling', 'sad', 'and', 'sluggish', 'for', 'no', 'reason', 'the', 'smallest', 'annoyance', 'would', 'set', 'me', 'off', 'and', 'make', 'my', 'heart', 'beat', 'faster', 'had', 'been', 'suffering', 'from', 'these', 'vacillations', 'from', 'anxiety', 'to', 'depression', 'for', 'decade', 'had', 'accepted', 'them', 'as', 'part', 'of', 'me', 'thought', 'was', 'just', 'an', 'overly', 'emotional', 'and', 'dramatic', 'person', 'when', 'february', 'rolled', 'around', 'realized', 'didn', 'want', 'to', 'start', 'drinking', 'again', 'had', 'stepped', 'back', 'from', 'partying', 'for', 'long', 'enough', 'to', 'see', 'how', 'it', 'had', 'negatively', 'impacted', 'me', 'didn', 'want', 'to', 'risk', 'losing', 'this', 'peace', 'had', 'just', 'started', 'to', 'discover', 'felt', 'small', 'pangs', 'of', 'fomo', 'as', 'summer', 'passed', 'and', 'saw', 'instagram', 'posts', 'of', 'friends', 'drinking', 'the', 'day', 'away', 'at', 'the', 'park', 'my', 'family', 'vacation', 'was', 'challenging', 'as', 'come', 'from', 'clan', 'of', 'enthusiastic', 'drinkers', 'as', 'my', 'family', 'members', 'delighted', 'in', 'enjoying', 'daytime', 'beach', 'cocktails', 'sometimes', 'felt', 'listless', 'what', 'was', 'supposed', 'to', 'do', 'with', 'myself', 'in', 'september', 'attended', 'my', 'first', 'big', 'concert', 'at', 'the', 'greek', 'theatre', 'since', 'had', 'stopped', 'drinking', 'being', 'in', 'the', 'loud', 'chilly', 'stadium', 'surrounded', 'by', 'enthusiastic', 'revelers', 'was', 'pretty', 'triggering', 'for', 'me', 'realized', 'there', 'are', 'some', 'pieces', 'of', 'the', 'concert', 'experience', 'that', 'can', 'make', 'one', 'uncomfortable', 'the', 'slow', 'melancholy', 'sounds', 'of', 'the', 'band', 'the', 'national', 'didn', 'help', 'contribute', 'to', 'my', 'vulnerable', 'mood', 'old', 'ali', 'would', 'make', 'sure', 'to', 'maintain', 'constant', 'buzz', 'at', 'concert', 'to', 'soften', 'the', 'annoyances', 'around', 'her', 'sober', 'ali', 'needed', 'to', 'learn', 'to', 'just', 'deal', 'with', 'it', 'as', 'the', 'months', 'passed', 'realized', 'the', 'only', 'time', 'really', 'had', 'the', 'urge', 'to', 'drink', 'was', 'when', 'was', 'upset', 'about', 'something', 'was', 'able', 'to', 'observe', 'how', 'had', 'been', 'using', 'alcohol', 'as', 'my', 'coping', 'mechanism', 'to', 'deal', 'with', 'any', 'part', 'of', 'life', 'that', 'made', 'me', 'sad', 'stressed', 'or', 'uncomfortable', 'taking', 'this', 'break', 'allowed', 'me', 'to', 'view', 'the', 'patterns', 'and', 'habits', 'had', 'developed', 'over', 'the', 'years', 'it', 'gave', 'me', 'the', 'perspective', 'needed', 'to', 'view', 'my', 'unhealthy', 'relationship', 'with', 'alcohol', 'it', 'led', 'me', 'to', 'realize', 'that', 'this', 'break', 'with', 'drinking', 'should', 'become', 'permanent', 'other', 'great', 'byproducts', 'from', 'quitting', 'booze', 'used', 'to', 'have', 'to', 'take', 'allergy', 'medicine', 'every', 'night', 'but', 'my', 'allergies', 'went', 'away', 'replacing', 'drinking', 'time', 'with', 'yoga', 'classes', 'has', 'toned', 'and', 'strengthened', 'my', 'body', 'in', 'ways', 'didn', 'realize', 'were', 'possible', 'can', 'feel', 'that', 'am', 'in', 'the', 'best', 'athletic', 'shape', 'of', 'my', 'life', 'my', 'skin', 'has', 'aged', 'backwards', 'about', 'five', 'years', 'and', 'looks', 'amazing', 'fall', 'asleep', 'as', 'soon', 'as', 'my', 'head', 'hits', 'the', 'pillow', 'and', 'sleep', 'soundly', 'for', 'eight', 'hours', 'and', 'bounce', 'awake', 'well', 'rested', 'frequently', 'enjoy', 'watching', 'the', 'sunrise', 'no', 'hangovers', 'feel', 'like', 'have', 'so', 'much', 'more', 'time', 'to', 'accomplish', 'things', 'want', 'to', 'do', 'feel', 'like', 'my', 'brain', 'is', 'more', 'clear', 'and', 'functioning', 'at', 'higher', 'level', 'my', 'patience', 'has', 'dramatically', 'increased', 'my', 'ability', 'to', 'focus', 'and', 'connect', 'with', 'my', 'friends', 'and', 'clients', 'has', 'intensified', 'was', 'challenging', 'separated', 'from', 'my', 'husband', 'after', 'eleven', 'years', 'together', 'moved', 'out', 'of', 'the', 'charming', 'north', 'beach', 'apartment', 'had', 'lived', 'in', 'with', 'him', 'for', 'decade', 'suffered', 'shoulder', 'injury', 'on', 'muni', 'that', 'will', 'have', 'to', 'deal', 'with', 'for', 'the', 'rest', 'of', 'my', 'life', 'found', 'out', 'my', 'beloved', 'salon', 'was', 'closing', 'down', 'and', 'needed', 'to', 'find', 'new', 'place', 'to', 'work', 'have', 'gone', 'through', 'so', 'much', 'in', 'my', 'personal', 'life', 'this', 'year', 'can', 'imagine', 'how', 'hard', 'this', 'year', 'would', 'have', 'been', 'if', 'was', 'still', 'drinking', 'and', 'dealing', 'with', 'the', 'vicious', 'cycle', 'of', 'hangovers', 'and', 'mood', 'swings', 'every', 'day', 'remind', 'myself', 'that', 'brave', 'strong', 'not', 'afraid', 'to', 'face', 'new', 'challenges', 'when', 'share', 'my', 'experience', 'with', 'friends', 'and', 'clients', 'they', 'often', 'ask', 'if', 'quitting', 'drinking', 'has', 'changed', 'my', 'social', 'life', 'always', 'reply', 'yes', 'had', 'been', 'going', 'hard', 'for', 'decade', 'didn', 'realize', 'it', 'but', 'was', 'ready', 'for', 'more', 'restful', 'life', 'still', 'see', 'my', 'friends', 'just', 'in', 'the', 'context', 'of', 'casual', 'dinners', 'at', 'home', 'yoga', 'classes', 'book', 'clubs', 'and', 'walks', 'in', 'the', 'park', 'go', 'out', 'once', 'in', 'while', 'and', 'still', 'enjoy', 'socializing', 'don', 'find', 'it', 'difficult', 'or', 'tempting', 'to', 'be', 'around', 'drinking', 'because', 'feel', 'so', 'amazing', 'wouldn', 'trade', 'it', 'do', 'miss', 'red', 'wine', 'when', 'attending', 'book', 'club', 'or', 'dining', 'at', 'nice', 'restaurant', 'but', 'friendly', 'so', 'll', 'still', 'take', 'the', 'edge', 'off', 'with', 'cannabis', 'here', 'and', 'there', 'when', 'feel', 'the', 'urge', 'to', 'imbibe', 'wanted', 'to', 'share', 'this', 'experience', 'in', 'case', 'can', 'be', 'helpful', 'have', 'found', 'it', 'so', 'valuable', 'to', 'hear', 'about', 'others', 'struggles', 'with', 'mental', 'health', 'issues', 'and', 'if', 'you', 'have', 'similar', 'challenges', 'you', 'are', 'not', 'alone', 'truly', 'didn', 'realize', 'how', 'negatively', 'my', 'drinking', 'habits', 'were', 'impacting', 'my', 'mental', 'health', 'until', 'decided', 'to', 'take', 'break', 'so', 'grateful', 'made', 'this', 'decision', 'and', 'so', 'proud', 'of', 'myself', 'that', 'have', 'made', 'it', 'whole', 'year', 'without', 'alcohol', 'turn', 'in', 'few', 'weeks', 'and', 'am', 'excited', 'to', 'see', 'what', 'this', 'new', 'year', 'will', 'bring', 'for', 'this', 'new', 'ali']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'year', 'without', 'boozedrinking', 'was', 'my', 'routine', 'booze', 'was', 'my', 'pattern', 'red_wine', 'while', 'cooked', 'dinner', 'whiskey', 'while', 'got', 'ready', 'to', 'go', 'out', 'more', 'wine', 'for', 'an', 'afternoon', 'in', 'the', 'park', 'every', 'occasion', 'revolved_around', 'drinking', 'felt', 'fatigued', 'around', 'the', 'holidays', 'many', 'peers', 'started', 'talking', 'about', 'doing', 'sober', 'january', 'the', 'first', 'thought', 'in', 'my', 'head', 'was', 'wow', 'that', 'sounds', 'really', 'hard', 'that', 'thought', 'led', 'to', 'the', 'realization', 'that', 'if', 'it', 'seemed', 'hard', 'maybe', 'it', 'was', 'necessary', 'this', 'time', 'last', 'year', 'decided', 'to', 'attempt', 'sober', 'january', 'as', 'january', 'started', 'yoga', 'classes', 'replaced', 'the', 'hours', 'would', 'have', 'spent', 'drinking', 'with', 'friends', 'spent', 'my', 'th_birthday', 'on', 'saturday', 'january_th', 'organizing', 'my', 'kitchen', 'and', 'listening', 'to', 'podcasts', 'felt', 'myself', 'drawn', 'to', 'more', 'philosophical', 'content', 'such', 'as', 'on', 'being', 'by', 'krista', 'tippett', 'and', 'couldn', 'get', 'enough', 'social', 'justice', 'news', 'from', 'pod', 'save', 'the', 'people', 'by', 'deray', 'mckesson', 'read', 'books', 'even', 'more', 'voraciously', 'than', 'before', 'three', 'favorites', 'from', 'the', 'start', 'of', 'were', 'sing', 'unburied', 'sing', 'by', 'jesmyn', 'ward', 'red', 'clocks', 'by', 'leni', 'zumas', 'and', 'the', 'immortalists', 'by', 'chloe', 'benjamin', 'you', 'can', 'see', 'my', 'complete', 'reading', 'list', 'from', 'books', 'total', 'on', 'my', 'goodreads', 'account', 'rested', 'more', 'isolated', 'myself', 'and', 'kept', 'myself', 'busy', 'so', 'wouldn', 'be', 'tempted', 'to', 'drink', 'after', 'three', 'weeks', 'with', 'no', 'alcohol', 'about', 'percent', 'of', 'my', 'anxiety', 'and', 'depression', 'symptoms', 'went', 'away', 'everyday', 'used', 'to', 'include', 'erratic', 'mood_swings', 'would', 'feel', 'elated', 'and', 'edgy', 'for', 'few', 'hours', 'only', 'to', 'crash', 'down', 'to', 'feeling', 'sad', 'and', 'sluggish', 'for', 'no', 'reason', 'the', 'smallest', 'annoyance', 'would', 'set', 'me', 'off', 'and', 'make', 'my', 'heart', 'beat', 'faster', 'had', 'been', 'suffering', 'from', 'these', 'vacillations', 'from', 'anxiety', 'to', 'depression', 'for', 'decade', 'had', 'accepted', 'them', 'as', 'part', 'of', 'me', 'thought', 'was', 'just', 'an', 'overly', 'emotional', 'and', 'dramatic', 'person', 'when', 'february', 'rolled', 'around', 'realized', 'didn', 'want', 'to', 'start', 'drinking', 'again', 'had', 'stepped', 'back', 'from', 'partying', 'for', 'long', 'enough', 'to', 'see', 'how', 'it', 'had', 'negatively_impacted', 'me', 'didn', 'want', 'to', 'risk', 'losing', 'this', 'peace', 'had', 'just', 'started', 'to', 'discover', 'felt', 'small', 'pangs', 'of', 'fomo', 'as', 'summer', 'passed', 'and', 'saw', 'instagram', 'posts', 'of', 'friends', 'drinking', 'the', 'day', 'away', 'at', 'the', 'park', 'my', 'family', 'vacation', 'was', 'challenging', 'as', 'come', 'from', 'clan', 'of', 'enthusiastic', 'drinkers', 'as', 'my', 'family_members', 'delighted', 'in', 'enjoying', 'daytime', 'beach', 'cocktails', 'sometimes', 'felt', 'listless', 'what', 'was', 'supposed', 'to', 'do', 'with', 'myself', 'in', 'september', 'attended', 'my', 'first', 'big', 'concert', 'at', 'the', 'greek', 'theatre', 'since', 'had', 'stopped', 'drinking', 'being', 'in', 'the', 'loud', 'chilly', 'stadium', 'surrounded', 'by', 'enthusiastic', 'revelers', 'was', 'pretty', 'triggering', 'for', 'me', 'realized', 'there', 'are', 'some', 'pieces', 'of', 'the', 'concert', 'experience', 'that', 'can', 'make', 'one', 'uncomfortable', 'the', 'slow', 'melancholy', 'sounds', 'of', 'the', 'band', 'the', 'national', 'didn', 'help', 'contribute', 'to', 'my', 'vulnerable', 'mood', 'old', 'ali', 'would', 'make', 'sure', 'to', 'maintain', 'constant', 'buzz', 'at', 'concert', 'to', 'soften', 'the', 'annoyances', 'around', 'her', 'sober', 'ali', 'needed', 'to', 'learn', 'to', 'just', 'deal', 'with', 'it', 'as', 'the', 'months', 'passed', 'realized', 'the', 'only', 'time', 'really', 'had', 'the', 'urge', 'to', 'drink', 'was', 'when', 'was', 'upset', 'about', 'something', 'was', 'able', 'to', 'observe', 'how', 'had', 'been', 'using', 'alcohol', 'as', 'my', 'coping_mechanism', 'to', 'deal', 'with', 'any', 'part', 'of', 'life', 'that', 'made', 'me', 'sad', 'stressed', 'or', 'uncomfortable', 'taking', 'this', 'break', 'allowed', 'me', 'to', 'view', 'the', 'patterns', 'and', 'habits', 'had', 'developed', 'over', 'the', 'years', 'it', 'gave', 'me', 'the', 'perspective', 'needed', 'to', 'view', 'my', 'unhealthy', 'relationship', 'with', 'alcohol', 'it', 'led', 'me', 'to', 'realize', 'that', 'this', 'break', 'with', 'drinking', 'should', 'become', 'permanent', 'other', 'great', 'byproducts', 'from', 'quitting', 'booze', 'used', 'to', 'have', 'to', 'take', 'allergy', 'medicine', 'every', 'night', 'but', 'my', 'allergies', 'went', 'away', 'replacing', 'drinking', 'time', 'with', 'yoga', 'classes', 'has', 'toned', 'and', 'strengthened', 'my', 'body', 'in', 'ways', 'didn', 'realize', 'were', 'possible', 'can', 'feel', 'that', 'am', 'in', 'the', 'best', 'athletic', 'shape', 'of', 'my', 'life', 'my', 'skin', 'has', 'aged', 'backwards', 'about', 'five', 'years', 'and', 'looks', 'amazing', 'fall_asleep', 'as', 'soon', 'as', 'my', 'head', 'hits', 'the', 'pillow', 'and', 'sleep', 'soundly', 'for', 'eight', 'hours', 'and', 'bounce', 'awake', 'well', 'rested', 'frequently', 'enjoy', 'watching', 'the', 'sunrise', 'no', 'hangovers', 'feel', 'like', 'have', 'so', 'much', 'more', 'time', 'to', 'accomplish', 'things', 'want', 'to', 'do', 'feel', 'like', 'my', 'brain', 'is', 'more', 'clear', 'and', 'functioning', 'at', 'higher', 'level', 'my', 'patience', 'has', 'dramatically', 'increased', 'my', 'ability', 'to', 'focus', 'and', 'connect', 'with', 'my', 'friends', 'and', 'clients', 'has', 'intensified', 'was', 'challenging', 'separated', 'from', 'my', 'husband', 'after', 'eleven', 'years', 'together', 'moved', 'out', 'of', 'the', 'charming', 'north', 'beach', 'apartment', 'had', 'lived', 'in', 'with', 'him', 'for', 'decade', 'suffered', 'shoulder', 'injury', 'on', 'muni', 'that', 'will', 'have', 'to', 'deal', 'with', 'for', 'the', 'rest', 'of', 'my', 'life', 'found', 'out', 'my', 'beloved', 'salon', 'was', 'closing', 'down', 'and', 'needed', 'to', 'find', 'new', 'place', 'to', 'work', 'have', 'gone', 'through', 'so', 'much', 'in', 'my', 'personal', 'life', 'this', 'year', 'can', 'imagine', 'how', 'hard', 'this', 'year', 'would', 'have', 'been', 'if', 'was', 'still', 'drinking', 'and', 'dealing', 'with', 'the', 'vicious_cycle', 'of', 'hangovers', 'and', 'mood_swings', 'every', 'day', 'remind', 'myself', 'that', 'brave', 'strong', 'not', 'afraid', 'to', 'face', 'new', 'challenges', 'when', 'share', 'my', 'experience', 'with', 'friends', 'and', 'clients', 'they', 'often', 'ask', 'if', 'quitting', 'drinking', 'has', 'changed', 'my', 'social', 'life', 'always', 'reply', 'yes', 'had', 'been', 'going', 'hard', 'for', 'decade', 'didn', 'realize', 'it', 'but', 'was', 'ready', 'for', 'more', 'restful', 'life', 'still', 'see', 'my', 'friends', 'just', 'in', 'the', 'context', 'of', 'casual', 'dinners', 'at', 'home', 'yoga', 'classes', 'book', 'clubs', 'and', 'walks', 'in', 'the', 'park', 'go', 'out', 'once', 'in', 'while', 'and', 'still', 'enjoy', 'socializing', 'don', 'find', 'it', 'difficult', 'or', 'tempting', 'to', 'be', 'around', 'drinking', 'because', 'feel', 'so', 'amazing', 'wouldn_trade', 'it', 'do', 'miss', 'red_wine', 'when', 'attending', 'book', 'club', 'or', 'dining', 'at', 'nice', 'restaurant', 'but', 'friendly', 'so', 'll', 'still', 'take', 'the', 'edge', 'off', 'with', 'cannabis', 'here', 'and', 'there', 'when', 'feel', 'the', 'urge', 'to', 'imbibe', 'wanted', 'to', 'share', 'this', 'experience', 'in', 'case', 'can', 'be', 'helpful', 'have', 'found', 'it', 'so', 'valuable', 'to', 'hear', 'about', 'others', 'struggles', 'with', 'mental_health_issues', 'and', 'if', 'you', 'have', 'similar', 'challenges', 'you', 'are', 'not', 'alone', 'truly', 'didn', 'realize', 'how', 'negatively', 'my', 'drinking', 'habits', 'were', 'impacting', 'my', 'mental_health', 'until', 'decided', 'to', 'take', 'break', 'so', 'grateful', 'made', 'this', 'decision', 'and', 'so', 'proud', 'of', 'myself', 'that', 'have', 'made', 'it', 'whole', 'year', 'without', 'alcohol', 'turn', 'in', 'few', 'weeks', 'and', 'am', 'excited', 'to', 'see', 'what', 'this', 'new', 'year', 'will', 'bring', 'for', 'this', 'new', 'ali']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['year', 'boozedrinke', 'routine', 'booze', 'pattern', 'cook', 'dinner', 'whiskey', 'get', 'ready', 'go', 'wine', 'afternoon', 'park', 'occasion', 'revolved_around', 'drinking', 'feel', 'fatigued', 'holiday', 'many', 'peer', 'start', 'talk', 'sober', 'january', 'first', 'think', 'head', 'sound', 'really', 'hard', 'thought', 'lead', 'realization', 'seem', 'hard', 'maybe', 'necessary', 'time', 'last', 'year', 'decide', 'attempt', 'sober', 'january', 'january', 'start', 'yoga', 'class', 'replace', 'hour', 'spend', 'drink', 'friend', 'spend', 'th_birthday', 'saturday', 'organizing', 'kitchen', 'listening', 'podcast', 'feel', 'draw', 'philosophical', 'content', 'krista', 'tippett', 'get', 'enough', 'social', 'justice', 'news', 'pod', 'save', 'people', 'deray', 'mckesson', 'read', 'book', 'even', 'voraciously', 'favorite', 'start', 'sing', 'unburied', 'sing', 'jesmyn', 'ward', 'red', 'clock', 'leni', 'zumas', 'immortalist', 'chloe', 'benjamin', 'see', 'complete', 'reading', 'list', 'book', 'total', 'goodread', 'account', 'rest', 'isolate', 'keep', 'busy', 'tempt', 'drink', 'week', 'alcohol', 'percent', 'anxiety', 'depression', 'symptom', 'go', 'away', 'everyday', 'use', 'include', 'erratic', 'mood_swing', 'feel', 'elated', 'edgy', 'hour', 'crash', 'feel', 'sad', 'sluggish', 'reason', 'small', 'annoyance', 'set', 'make', 'heart', 'beat', 'fast', 'suffer', 'vacillation', 'anxiety', 'depression', 'decade', 'accept', 'part', 'think', 'overly', 'emotional', 'dramatic', 'person', 'february', 'roll', 'around', 'realize', 'want', 'start', 'drink', 'step', 'back', 'party', 'long', 'enough', 'negatively_impacted', 'want', 'risk', 'lose', 'peace', 'start', 'discover', 'feel', 'small', 'pang', 'fomo', 'summer', 'pass', 'see', 'instagram', 'post', 'friend', 'drink', 'day', 'away', 'park', 'family', 'vacation', 'challenge', 'come', 'clan', 'enthusiastic', 'drinker', 'family_member', 'delight', 'enjoy', 'daytime', 'beach', 'cocktail', 'sometimes', 'feel', 'listless', 'suppose', 'september', 'attend', 'first', 'big', 'concert', 'greek', 'theatre', 'stop', 'drink', 'loud', 'chilly', 'stadium', 'surround', 'enthusiastic', 'reveler', 'pretty', 'trigger', 'realize', 'piece', 'concert', 'experience', 'make', 'uncomfortable', 'slow', 'melancholy', 'sound', 'band', 'national', 'help', 'contribute', 'vulnerable', 'mood', 'old', 'ali', 'make', 'sure', 'maintain', 'constant', 'buzz', 'concert', 'soften', 'annoyance', 'sober', 'ali', 'need', 'learn', 'deal', 'month', 'realize', 'time', 'really', 'urge', 'drink', 'upset', 'able', 'observe', 'use', 'alcohol', 'coping_mechanism', 'deal', 'part', 'life', 'make', 'sad', 'stress', 'uncomfortable', 'taking', 'break', 'allow', 'view', 'pattern', 'habit', 'develop', 'year', 'give', 'perspective', 'need', 'view', 'unhealthy', 'relationship', 'alcohol', 'lead', 'realize', 'break', 'drinking', 'become', 'permanent', 'great', 'byproduct', 'quit', 'booze', 'use', 'take', 'allergy', 'medicine', 'night', 'allergy', 'go', 'away', 'replace', 'drinking', 'time', 'yoga', 'class', 'tone', 'strengthen', 'body', 'way', 'realize', 'possible', 'feel', 'good', 'athletic', 'shape', 'life', 'skin', 'age', 'backwards', 'year', 'look', 'amazing', 'soon', 'head', 'hit', 'pillow', 'sleep', 'soundly', 'hour', 'bounce', 'awake', 'well', 'rest', 'frequently', 'watch', 'sunrise', 'hangover', 'feel', 'much', 'time', 'accomplish', 'thing', 'want', 'feel', 'brain', 'clear', 'function', 'high', 'level', 'patience', 'dramatically', 'increase', 'ability', 'focus', 'connect', 'friend', 'client', 'intensify', 'challenge', 'separate', 'husband', 'year', 'together', 'move', 'charming', 'north', 'beach', 'apartment', 'live', 'decade', 'suffer', 'shoulder', 'injury', 'muni', 'deal', 'rest', 'life', 'find', 'beloved', 'salon', 'closing', 'need', 'find', 'new', 'place', 'work', 'go', 'much', 'personal', 'life', 'year', 'imagine', 'hard', 'year', 'still', 'drinking', 'deal', 'vicious_cycle', 'hangover', 'mood_swing', 'day', 'remind', 'brave', 'strong', 'afraid', 'face', 'new', 'challenge', 'share', 'experience', 'friend', 'client', 'often', 'ask', 'quit', 'drinking', 'change', 'social', 'life', 'always', 'reply', 'go', 'hard', 'decade', 'realize', 'ready', 'restful', 'life', 'still', 'see', 'friend', 'context', 'casual', 'dinner', 'home', 'yoga', 'class', 'book', 'club', 'walk', 'park', 'go', 'still', 'enjoy', 'find', 'difficult', 'tempting', 'drinking', 'feel', 'amazing', 'trade', 'miss', 'attend', 'book', 'club', 'dine', 'nice', 'restaurant', 'friendly', 'still', 'take', 'edge', 'cannabis', 'feel', 'urge', 'imbibe', 'want', 'share', 'experience', 'case', 'helpful', 'find', 'valuable', 'hear', 'other', 'struggle', 'issue', 'similar', 'challenge', 'alone', 'truly', 'realize', 'negatively', 'drink', 'habit', 'impact', 'mental_health', 'decide', 'take', 'break', 'grateful', 'make', 'decision', 'proud', 'make', 'whole', 'year', 'alcohol', 'turn', 'week', 'excited', 'see', 'new', 'year', 'bring', 'new', 'ali']]\n",
      "There are 3054 unique words in the dictionary, 3054 remain after filtering out lest frequent.\n",
      "3054 remain after filtering out most commonly used words based on tfidf scores.\n"
     ]
    }
   ],
   "source": [
    "#----- CHANGED ------#\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# 2. Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# 3. Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# 4. Term Document Frequency and Create a bag of words\n",
    "bow_corpus = bow(dictionary=id2word, processed_docs=texts)\n",
    "\n",
    "# 5. Calculate low_tfidf_words\n",
    "# Keep only words with tfidf ranking <= x * len(dictionary)\n",
    "x = 0.2\n",
    "total_word_count, DictDocFreq = tf_df(bow_corpus, id2word)\n",
    "sorted_TFIDF = sort_tfidf(bow_corpus, total_word_count, DictDocFreq)\n",
    "low_tfidf_words = get_low_tfidf_words(x, id2word, sorted_TFIDF)\n",
    "\n",
    "# 6. Filter out least frequently used words\n",
    "no_below = 0.01\n",
    "keep_n = 10000\n",
    "dict_least_freq_filtered = filter_least_frequent(id2word, texts, \n",
    "                                                 no_below, keep_n)\n",
    "\n",
    "# 7. Filter out most commonly used words (i.e. words with low TF-IDF score)\n",
    "dict_tfidf_filtered = filter_most_common(dict_least_freq_filtered, low_tfidf_words)\n",
    "\n",
    "# 8. Create the second bag of words - bow_corpus_TFIDFfiltered, \n",
    "# created after least frequently and most commonly used words were filtered out.\n",
    "corpus = bow(dict_tfidf_filtered, texts)\n",
    "\n",
    "# View\n",
    "[[(dict_tfidf_filtered[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dict_tfidf_filtered,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25,\n",
      "  '0.174*\"god\" + 0.072*\"faith\" + 0.045*\"plane\" + 0.037*\"pray\" + 0.035*\"peace\" '\n",
      "  '+ 0.035*\"church\" + 0.033*\"prayer\" + 0.028*\"fly\" + 0.028*\"lord\" + '\n",
      "  '0.024*\"christian\"'),\n",
      " (11,\n",
      "  '0.186*\"dog\" + 0.149*\"job\" + 0.078*\"cat\" + 0.062*\"pet\" + 0.056*\"interview\" + '\n",
      "  '0.041*\"animal\" + 0.037*\"company\" + 0.029*\"america\" + 0.028*\"owner\" + '\n",
      "  '0.027*\"employee\"'),\n",
      " (21,\n",
      "  '0.081*\"therapy\" + 0.079*\"therapist\" + 0.075*\"student\" + 0.060*\"school\" + '\n",
      "  '0.050*\"class\" + 0.039*\"college\" + 0.026*\"university\" + 0.026*\"session\" + '\n",
      "  '0.026*\"teacher\" + 0.023*\"exam\"'),\n",
      " (23,\n",
      "  '0.146*\"disorder\" + 0.094*\"symptom\" + 0.039*\"social\" + 0.023*\"treatment\" + '\n",
      "  '0.023*\"cause\" + 0.022*\"person\" + 0.021*\"experience\" + 0.020*\"suffer\" + '\n",
      "  '0.019*\"common\" + 0.017*\"condition\"'),\n",
      " (14,\n",
      "  '0.086*\"medication\" + 0.073*\"doctor\" + 0.051*\"patient\" + 0.047*\"treatment\" + '\n",
      "  '0.038*\"drug\" + 0.028*\"therapy\" + 0.025*\"pill\" + 0.025*\"treat\" + '\n",
      "  '0.024*\"pain\" + 0.023*\"medical\"'),\n",
      " (5,\n",
      "  '0.055*\"book\" + 0.039*\"goal\" + 0.039*\"business\" + 0.034*\"decision\" + '\n",
      "  '0.031*\"failure\" + 0.029*\"value\" + 0.029*\"coach\" + 0.027*\"career\" + '\n",
      "  '0.025*\"money\" + 0.024*\"success\"'),\n",
      " (20,\n",
      "  '0.275*\"worry\" + 0.116*\"problem\" + 0.054*\"anxious\" + 0.030*\"future\" + '\n",
      "  '0.029*\"solve\" + 0.028*\"overthinke\" + 0.025*\"solution\" + 0.023*\"worried\" + '\n",
      "  '0.019*\"concern\" + 0.017*\"happen\"'),\n",
      " (19,\n",
      "  '0.044*\"joy\" + 0.036*\"love\" + 0.034*\"gift\" + 0.032*\"nature\" + 0.028*\"create\" '\n",
      "  '+ 0.021*\"energy\" + 0.021*\"art\" + 0.020*\"creative\" + 0.018*\"space\" + '\n",
      "  '0.017*\"peace\"'),\n",
      " (9,\n",
      "  '0.052*\"com\" + 0.048*\"social_media\" + 0.038*\"post\" + 0.038*\"online\" + '\n",
      "  '0.032*\"instagram\" + 0.031*\"app\" + 0.029*\"video\" + 0.028*\"phone\" + '\n",
      "  '0.027*\"facebook\" + 0.022*\"content\"'),\n",
      " (1,\n",
      "  '0.219*\"fear\" + 0.079*\"panic_attack\" + 0.057*\"panic\" + 0.046*\"attack\" + '\n",
      "  '0.039*\"brain\" + 0.034*\"trigger\" + 0.032*\"ocd\" + 0.027*\"afraid\" + '\n",
      "  '0.020*\"threat\" + 0.017*\"danger\"'),\n",
      " (10,\n",
      "  '0.107*\"stress\" + 0.063*\"body\" + 0.024*\"exercise\" + 0.018*\"physical\" + '\n",
      "  '0.018*\"health\" + 0.018*\"reduce\" + 0.017*\"mindfulness\" + 0.016*\"yoga\" + '\n",
      "  '0.015*\"also\" + 0.014*\"calm\"'),\n",
      " (18,\n",
      "  '0.026*\"heart\" + 0.025*\"body\" + 0.019*\"breathe\" + 0.016*\"mind\" + 0.015*\"run\" '\n",
      "  '+ 0.015*\"hand\" + 0.014*\"breath\" + 0.014*\"hold\" + 0.012*\"fight\" + '\n",
      "  '0.012*\"pain\"'),\n",
      " (13,\n",
      "  '0.058*\"friend\" + 0.052*\"talk\" + 0.042*\"ask\" + 0.026*\"speak\" + '\n",
      "  '0.024*\"person\" + 0.024*\"social\" + 0.024*\"tell\" + 0.021*\"question\" + '\n",
      "  '0.020*\"conversation\" + 0.018*\"share\"'),\n",
      " (12,\n",
      "  '0.047*\"year\" + 0.023*\"home\" + 0.020*\"month\" + 0.019*\"week\" + 0.017*\"back\" + '\n",
      "  '0.017*\"first\" + 0.015*\"tell\" + 0.014*\"last\" + 0.014*\"still\" + 0.014*\"hour\"'),\n",
      " (8,\n",
      "  '0.023*\"look\" + 0.017*\"walk\" + 0.013*\"room\" + 0.011*\"back\" + 0.011*\"turn\" + '\n",
      "  '0.010*\"smile\" + 0.010*\"eye\" + 0.009*\"leave\" + 0.009*\"sit\" + 0.008*\"light\"'),\n",
      " (22,\n",
      "  '0.043*\"live\" + 0.036*\"world\" + 0.030*\"never\" + 0.017*\"always\" + '\n",
      "  '0.016*\"moment\" + 0.014*\"believe\" + 0.014*\"hope\" + 0.014*\"ever\" + '\n",
      "  '0.014*\"happy\" + 0.014*\"lose\"'),\n",
      " (6,\n",
      "  '0.023*\"experience\" + 0.013*\"also\" + 0.010*\"many\" + 0.009*\"however\" + '\n",
      "  '0.009*\"often\" + 0.007*\"state\" + 0.007*\"form\" + 0.007*\"issue\" + '\n",
      "  '0.006*\"example\" + 0.006*\"follow\"'),\n",
      " (3,\n",
      "  '0.023*\"start\" + 0.017*\"new\" + 0.014*\"change\" + 0.013*\"keep\" + 0.012*\"year\" '\n",
      "  '+ 0.011*\"list\" + 0.011*\"give\" + 0.011*\"many\" + 0.010*\"spend\" + '\n",
      "  '0.009*\"also\"'),\n",
      " (17,\n",
      "  '0.026*\"write\" + 0.024*\"really\" + 0.015*\"read\" + 0.014*\"start\" + '\n",
      "  '0.014*\"maybe\" + 0.013*\"lot\" + 0.013*\"bad\" + 0.013*\"right\" + 0.012*\"little\" '\n",
      "  '+ 0.010*\"still\"'),\n",
      " (0,\n",
      "  '0.020*\"feeling\" + 0.017*\"self\" + 0.016*\"control\" + 0.016*\"situation\" + '\n",
      "  '0.014*\"learn\" + 0.014*\"change\" + 0.014*\"thought\" + 0.013*\"anxious\" + '\n",
      "  '0.011*\"happen\" + 0.010*\"become\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.810747149991645\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4765324078666899\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feeling', 'self', 'control', 'situation', 'learn', 'change', 'thought', 'anxious', 'happen', 'become']\n",
      "['fear', 'panic_attack', 'panic', 'attack', 'brain', 'trigger', 'ocd', 'afraid', 'threat', 'danger']\n",
      "['brain', 'study', 'increase', 'effect', 'research', 'show', 'also', 'technology', 'use', 'report']\n",
      "['start', 'new', 'change', 'keep', 'year', 'list', 'give', 'many', 'spend', 'also']\n",
      "['child', 'kid', 'parent', 'school', 'family', 'mother', 'young', 'mom', 'year', 'old']\n",
      "['book', 'goal', 'business', 'decision', 'failure', 'value', 'coach', 'career', 'money', 'success']\n",
      "['experience', 'also', 'many', 'however', 'often', 'state', 'form', 'issue', 'example', 'follow']\n",
      "['love', 'relationship', 'self', 'woman', 'care', 'man', 'trauma', 'pain', 'partner', 'emotional']\n",
      "['look', 'walk', 'room', 'back', 'turn', 'smile', 'eye', 'leave', 'sit', 'light']\n",
      "['com', 'social_media', 'post', 'online', 'instagram', 'app', 'video', 'phone', 'facebook', 'content']\n",
      "['stress', 'body', 'exercise', 'physical', 'health', 'reduce', 'mindfulness', 'yoga', 'also', 'calm']\n",
      "['dog', 'job', 'cat', 'pet', 'interview', 'animal', 'company', 'america', 'owner', 'employee']\n",
      "['year', 'home', 'month', 'week', 'back', 'first', 'tell', 'last', 'still', 'hour']\n",
      "['friend', 'talk', 'ask', 'speak', 'person', 'social', 'tell', 'question', 'conversation', 'share']\n",
      "['medication', 'doctor', 'patient', 'treatment', 'drug', 'therapy', 'pill', 'treat', 'pain', 'medical']\n",
      "['sleep', 'night', 'eat', 'music', 'bed', 'morning', 'drink', 'food', 'wake', 'hour']\n",
      "['thought', 'mind', 'meditation', 'practice', 'focus', 'moment', 'present', 'technique', 'minute', 'calm']\n",
      "['write', 'really', 'read', 'start', 'maybe', 'lot', 'bad', 'right', 'little', 'still']\n",
      "['heart', 'body', 'breathe', 'mind', 'run', 'hand', 'breath', 'hold', 'fight', 'pain']\n",
      "['joy', 'love', 'gift', 'nature', 'create', 'energy', 'art', 'creative', 'space', 'peace']\n",
      "['worry', 'problem', 'anxious', 'future', 'solve', 'overthinke', 'solution', 'worried', 'concern', 'happen']\n",
      "['therapy', 'therapist', 'student', 'school', 'class', 'college', 'university', 'session', 'teacher', 'exam']\n",
      "['live', 'world', 'never', 'always', 'moment', 'believe', 'hope', 'ever', 'happy', 'lose']\n",
      "['disorder', 'symptom', 'social', 'treatment', 'cause', 'person', 'experience', 'suffer', 'common', 'condition']\n",
      "['depression', 'mental_health', 'illness', 'struggle', 'issue', 'mental', 'mental_illness', 'suffer', 'depressed', 'talk']\n",
      "['god', 'faith', 'plane', 'pray', 'peace', 'church', 'prayer', 'fly', 'lord', 'christian']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,num_topics):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Year, Source, Topic_ID, Most_freq_words]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Year':[],'Source':[],'Topic_ID':[],'Most_freq_words':[]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Most_freq_words']=topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(Year=year)\n",
    "df = df.assign(Source='Medium')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for i in range(0,num_topics):\n",
    "    ls.append(i)\n",
    "df['Topic_ID']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Most_freq_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>[feeling, self, control, situation, learn, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1</td>\n",
       "      <td>[fear, panic_attack, panic, attack, brain, tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "      <td>[brain, study, increase, effect, research, sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3</td>\n",
       "      <td>[start, new, change, keep, year, list, give, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>[child, kid, parent, school, family, mother, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Source  Topic_ID                                    Most_freq_words\n",
       "0  2019  Medium         0  [feeling, self, control, situation, learn, cha...\n",
       "1  2019  Medium         1  [fear, panic_attack, panic, attack, brain, tri...\n",
       "2  2019  Medium         2  [brain, study, increase, effect, research, sho...\n",
       "3  2019  Medium         3  [start, new, change, keep, year, list, give, m...\n",
       "4  2019  Medium         4  [child, kid, parent, school, family, mother, y..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"topic_words_m2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
