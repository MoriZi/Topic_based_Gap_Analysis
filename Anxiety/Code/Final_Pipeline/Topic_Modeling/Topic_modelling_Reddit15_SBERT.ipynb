{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smriti/.local/lib/python3.5/site-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "df=pd.read_csv('/home/smriti/Desktop/NLP/MITACS/Anxiety/Data/CSV/Reddit/rAnxiety15.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#getting rid of deleted values\n",
    "df['Text']=df['Text'].replace('[deleted]','')\n",
    "#Combining title and text\n",
    "df[\"Post\"] = df[\"Title\"] + df[\"Text\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['Title', 'Text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>2015-07-21 04:49:57 EDT-0400</td>\n",
       "      <td>Does anyone else cry when they get yelled at o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>2015-07-19 18:41:29 EDT-0400</td>\n",
       "      <td>Whenever I go out the next morning I feel like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-07-16 00:12:44 EDT-0400</td>\n",
       "      <td>Does anyone else shake literally from anxiety?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>2015-07-17 20:43:37 EDT-0400</td>\n",
       "      <td>**Breakthrough** Just want to share my realiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2015-07-21 21:14:58 EDT-0400</td>\n",
       "      <td>I just discovered this subreddit and I already...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Comments                   Date Posted  \\\n",
       "0                 125  2015-07-21 04:49:57 EDT-0400   \n",
       "1                  36  2015-07-19 18:41:29 EDT-0400   \n",
       "2                  27  2015-07-16 00:12:44 EDT-0400   \n",
       "3                  34  2015-07-17 20:43:37 EDT-0400   \n",
       "4                  11  2015-07-21 21:14:58 EDT-0400   \n",
       "\n",
       "                                                Post  \n",
       "0  Does anyone else cry when they get yelled at o...  \n",
       "1  Whenever I go out the next morning I feel like...  \n",
       "2  Does anyone else shake literally from anxiety?...  \n",
       "3  **Breakthrough** Just want to share my realiza...  \n",
       "4  I just discovered this subreddit and I already...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Post.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['does', 'anyone', 'else', 'cry', 'when', 'they', 'get', 'yelled', 'at', 'or', 'when', 'someone', 'raises', 'their', 'voice', 'especially', 'when', 'its', 'teacher', 'or', 'something']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does_anyone_else', 'cry', 'when', 'they', 'get', 'yelled', 'at', 'or', 'when', 'someone', 'raises', 'their', 'voice', 'especially', 'when', 'its', 'teacher', 'or', 'something']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cry', 'yell', 'raise', 'voice', 'especially', 'teacher']]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=70,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(68,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (55,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (15,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (37,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (56,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (34,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (0,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (18,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (50,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (60,\n",
      "  '0.000*\"plummet\" + 0.000*\"damaging\" + 0.000*\"numerous\" + '\n",
      "  '0.000*\"frighteningly\" + 0.000*\"disconnect\" + 0.000*\"cheat\" + '\n",
      "  '0.000*\"mentality\" + 0.000*\"wide\" + 0.000*\"wherever\" + 0.000*\"primitive\"'),\n",
      " (17,\n",
      "  '0.145*\"therapist\" + 0.085*\"hospital\" + 0.080*\"suddenly\" + 0.071*\"pretty\" + '\n",
      "  '0.069*\"insurance\" + 0.044*\"mentally\" + 0.039*\"extreme\" + 0.037*\"nearly\" + '\n",
      "  '0.036*\"stranger\" + 0.028*\"generally\"'),\n",
      " (54,\n",
      "  '0.265*\"experience\" + 0.115*\"appointment\" + 0.072*\"type\" + 0.072*\"class\" + '\n",
      "  '0.040*\"semester\" + 0.036*\"professional\" + 0.028*\"memory\" + 0.026*\"solution\" '\n",
      "  '+ 0.024*\"hot\" + 0.023*\"strength\"'),\n",
      " (28,\n",
      "  '0.159*\"medication\" + 0.110*\"tomorrow\" + 0.045*\"note\" + 0.041*\"important\" + '\n",
      "  '0.038*\"comfortable\" + 0.038*\"literally\" + 0.031*\"anti\" + 0.031*\"doctor\" + '\n",
      "  '0.031*\"miss\" + 0.030*\"cbt\"'),\n",
      " (41,\n",
      "  '0.175*\"brain\" + 0.058*\"subreddit\" + 0.057*\"feel\" + 0.054*\"fix\" + '\n",
      "  '0.040*\"mistake\" + 0.035*\"sick\" + 0.035*\"ocd\" + 0.034*\"away\" + '\n",
      "  '0.033*\"guilty\" + 0.032*\"constantly\"'),\n",
      " (63,\n",
      "  '0.137*\"live\" + 0.125*\"can\" + 0.110*\"sometimes\" + 0.074*\"be\" + '\n",
      "  '0.072*\"fucking\" + 0.052*\"guess\" + 0.046*\"anymore\" + 0.042*\"die\" + '\n",
      "  '0.041*\"life\" + 0.029*\"girlfriend\"'),\n",
      " (13,\n",
      "  '0.165*\"disorder\" + 0.130*\"depression\" + 0.081*\"write\" + 0.054*\"definitely\" '\n",
      "  '+ 0.048*\"book\" + 0.037*\"hand\" + 0.037*\"create\" + 0.037*\"panic\" + '\n",
      "  '0.035*\"constantly\" + 0.026*\"suffer\"'),\n",
      " (31,\n",
      "  '0.127*\"small\" + 0.112*\"social\" + 0.064*\"put\" + 0.064*\"drink\" + '\n",
      "  '0.041*\"conversation\" + 0.040*\"mostly\" + 0.038*\"set\" + 0.035*\"coffee\" + '\n",
      "  '0.033*\"healthy\" + 0.033*\"positive\"'),\n",
      " (16,\n",
      "  '0.082*\"call\" + 0.041*\"away\" + 0.034*\"phone\" + 0.028*\"question\" + '\n",
      "  '0.027*\"put\" + 0.021*\"office\" + 0.021*\"answer\" + 0.020*\"visit\" + '\n",
      "  '0.020*\"especially\" + 0.020*\"sort\"'),\n",
      " (49,\n",
      "  '0.044*\"would\" + 0.027*\"say\" + 0.027*\"could\" + 0.024*\"give\" + 0.020*\"start\" '\n",
      "  '+ 0.017*\"still\" + 0.017*\"end\" + 0.016*\"get\" + 0.016*\"tell\" + '\n",
      "  '0.016*\"happen\"'),\n",
      " (52,\n",
      "  '0.043*\"be\" + 0.041*\"anxiety\" + 0.034*\"feel\" + 0.027*\"go\" + 0.020*\"get\" + '\n",
      "  '0.020*\"make\" + 0.017*\"time\" + 0.017*\"know\" + 0.016*\"people\" + 0.015*\"have\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -18.854276076062376\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3760334493216018\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 70\n",
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['play', 'online', 'game', 'name', 'email', 'daily', 'reply', 'later', 'sister', 'post']\n",
      "['simple', 'glad', 'accept', 'reality', 'one', 'function', 'relieve', 'occur', 'involve', 'frustrated']\n",
      "['red', 'letter', 'mail', 'advice', 'may', 'disconnect', 'mentality', 'frighteningly', 'wherever', 'numerous']\n",
      "['effect', 'beat', 'option', 'frequently', 'list', 'prolong', 'physical', 'cheat', 'primitive', 'peeve']\n",
      "['sleep', 'bed', 'night', 'noise', 'must', 'living', 'poop', 'hour', 'drink', 'soon']\n",
      "['inspire', 'life', 'cheat', 'disconnect', 'wide', 'mentality', 'peeve', 'placeconstant', 'numerous', 'frighteningly']\n",
      "['community', 'plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever']\n",
      "['ruin', 'wide', 'plummet', 'peeve', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'somethingive']\n",
      "['comment', 'rule', 'send', 'reminder', 'mentality', 'cheat', 'primitive', 'numerous', 'peeve', 'disconnect']\n",
      "['walk', 'study', 'horrible', 'lay', 'obsessive', 'holiday', 'article', 'breathe', 'socially', 'months_ago']\n",
      "['sorry', 'anyone_else', 'rant', 'tear', 'awesome', 'joke', 'terrify', 'potential', 'balance', 'bottle']\n",
      "['med', 'side', 'health', 'serious', 'seriously', 'counselor', 'ask', 'annoy', 'okay', 'possibly']\n",
      "['disorder', 'depression', 'write', 'definitely', 'book', 'hand', 'create', 'panic', 'constantly', 'suffer']\n",
      "['movie', 'battle', 'scene', 'esteem', 'quote', 'disconnect', 'primitive', 'cheat', 'peeve', 'numerous']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['call', 'away', 'phone', 'question', 'put', 'office', 'answer', 'visit', 'especially', 'sort']\n",
      "['therapist', 'hospital', 'suddenly', 'pretty', 'insurance', 'mentally', 'extreme', 'nearly', 'stranger', 'generally']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['need', 'plan', 'message', 'scream', 'condition', 'response', 'treatment', 'middle', 'seek', 'voice']\n",
      "['severe', 'music', 'certain', 'stick', 'listen', 'natural', 'surprise', 'desire', 'there', 'feature']\n",
      "['lately', 'whenever', 'totally', 'existence', 'quality', 'sound', 'humor', 'jealous', 'opinion', 'cute']\n",
      "['house', 'leave', 'home', 'room', 'store', 'light', 'throat', 'gag', 'pounding', 'leg']\n",
      "['schedule', 'routine', 'wide', 'plummet', 'peeve', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality']\n",
      "['hate', 'amazing', 'stupid', 'kill', 'edit', 'thank', 'rid', 'angry', 'drunk', 'experience']\n",
      "['com', 'proud', 'draw', 'sit', 'best', 'cheat', 'mentality', 'disconnect', 'peeve', 'frighteningly']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['outside', 'website', 'link', 'success', 'palpitation', 'victory', 'avoid', 'medicine', 'lexapro', 'virtual']\n",
      "['medication', 'tomorrow', 'note', 'important', 'comfortable', 'literally', 'anti', 'doctor', 'miss', 'cbt']\n",
      "['drive', 'upset', 'car', 'task', 'away', 'urge', 'welcome', 'driving', 'screw', 'technically']\n",
      "['wish', 'opposite', 'else', 'point', 'person', 'cheat', 'mentality', 'disconnect', 'peeve', 'frighteningly']\n",
      "['small', 'social', 'put', 'drink', 'conversation', 'mostly', 'set', 'coffee', 'healthy', 'positive']\n",
      "['ill', 'mom', 'bother', 'support', 'sad', 'funny', 'vent', 'carry', 'suicide', 'stuck']\n",
      "['crazy', 'rest', 'song', 'focused', 'adventure', 'scatter', 'pop', 'anxious', 'example', 'force']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['cope', 'terrifying', 'scare', 'disconnect', 'cheat', 'mentality', 'primitive', 'peeve', 'wherever', 'numerous']\n",
      "['prevent', 'plummet', 'placeconstant', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['text', 'reading', 'generalize', 'worst', 'mentality', 'cheat', 'frighteningly', 'disconnect', 'numerous', 'wherever']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['brain', 'subreddit', 'feel', 'fix', 'mistake', 'sick', 'ocd', 'away', 'guilty', 'constantly']\n",
      "['heart', 'breath', 'chest', 'deep', 'player', 'discomfort', 'take', 'anxiety', 'numerous', 'frighteningly']\n",
      "['finally', 'incredibly', 'drug', 'help', 'apologize', 'service', 'health', 'complete', 'doctor', 'bed']\n",
      "['free', 'post', 'helpful', 'weekend', 'view', 'recommend', 'thread', 'form', 'apartment', 'ability']\n",
      "['college', 'student', 'wide', 'mentality', 'cheat', 'plummet', 'numerous', 'frighteningly', 'damaging', 'disconnect']\n",
      "['relate', 'learn', 'slowly', 'light', 'uncomfortable', 'place', 'train', 'mindfulness', 'blanket', 'strange']\n",
      "['tend', 'entirely', 'wherever', 'plummet', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['would', 'say', 'could', 'give', 'start', 'still', 'end', 'get', 'tell', 'happen']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['effort', 'decrease', 'news', 'read', 'negative', 'anxiety', 'disconnect', 'frighteningly', 'mentality', 'peeve']\n",
      "['be', 'anxiety', 'feel', 'go', 'get', 'make', 'time', 'know', 'people', 'have']\n",
      "['parent', 'plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever']\n",
      "['experience', 'appointment', 'type', 'class', 'semester', 'professional', 'memory', 'solution', 'hot', 'strength']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['money', 'tip', 'reduce', 'yell', 'stomach', 'raise', 'teacher', 'butterfly', 'especially', 'mentality']\n",
      "['job', 'interview', 'lack', 'badly', 'position', 'crippling', 'peeve', 'numerous', 'survival', 'somethingive']\n",
      "['sweat', 'psychologist', 'awake', 'shake', 'wake', 'bed', 'literally', 'due', 'neurologist', 'start']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['body', 'blood', 'neck', 'somewhere', 'ready', 'walk', 'tense', 'mode', 'crash', 'nightmare']\n",
      "['shop', 'severely', 'tackle', 'buy', 'inside', 'numerous', 'disconnect', 'peeve', 'somethingive', 'primitive']\n",
      "['live', 'can', 'sometimes', 'be', 'fucking', 'guess', 'anymore', 'die', 'life', 'girlfriend']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['close', 'dark', 'fully', 'common', 'lead', 'fog', 'destructive', 'wrap', 'rough', 'behavior']\n",
      "['symptom', 'suppose', 'respond', 'amount', 'speech', 'disease', 'embarrassed', 'presentation', 'terrified', 'project']\n",
      "['fuck', 'city', 'edge', 'useless', 'square', 'answer', 'cheat', 'mentality', 'wherever', 'disconnect']\n",
      "['plummet', 'damaging', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide', 'wherever', 'primitive']\n",
      "['dumb', 'round', 'wherever', 'peeve', 'numerous', 'frighteningly', 'disconnect', 'cheat', 'mentality', 'wide']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,70):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most_freq_words</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Most_freq_words, Source, Topic_ID, Year]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Year':[],'Source':[],'Topic_ID':[],'Most_freq_words':[]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Most_freq_words']=topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Year='2015')\n",
    "df = df.assign(Source='Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for i in range(0,70):\n",
    "    ls.append(i)\n",
    "df['Topic_ID']=ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most_freq_words</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic_ID</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[plummet, damaging, numerous, frighteningly, d...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[play, online, game, name, email, daily, reply...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[simple, glad, accept, reality, one, function,...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[red, letter, mail, advice, may, disconnect, m...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[effect, beat, option, frequently, list, prolo...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Most_freq_words  Source  Topic_ID  Year\n",
       "0  [plummet, damaging, numerous, frighteningly, d...  Reddit         0  2015\n",
       "1  [play, online, game, name, email, daily, reply...  Reddit         1  2015\n",
       "2  [simple, glad, accept, reality, one, function,...  Reddit         2  2015\n",
       "3  [red, letter, mail, advice, may, disconnect, m...  Reddit         3  2015\n",
       "4  [effect, beat, option, frequently, list, prolo...  Reddit         4  2015"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"topic_words_r2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
