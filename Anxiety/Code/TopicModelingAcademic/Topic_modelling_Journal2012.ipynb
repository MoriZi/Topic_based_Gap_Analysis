{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtained Perplexity: -11.37, Coherence: 0.39, Best Number of Topics= 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from topic_model_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yxzh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "import os\n",
    "df=pd.read_csv('/Users/yxzh/PycharmProjects/GapAnalysis/Topic_based_Gap_Analysis/Anxiety/Data/CSV/Acad_2020.csv')\n",
    "#getting rid of NaN\n",
    "df=df.replace(np.nan, '', regex=True)\n",
    "#Combining title and text\n",
    "df[\"Text\"] = df[\"Title\"] + df[\"Abstract\"]\n",
    "#Now that we don't need Title or Text, we drop those columns before saving the file\n",
    "df=df.drop(['Title', 'Abstract'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreId</th>\n",
       "      <th>documentType</th>\n",
       "      <th>year</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.44597e+09</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>2020</td>\n",
       "      <td>Threat rapidly disrupts reward reversal learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.44597e+09</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>2020</td>\n",
       "      <td>Fear in the context of pain: Lessons learned f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.44597e+09</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>2020</td>\n",
       "      <td>The effects of positive interpretation bias on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.43522e+09</td>\n",
       "      <td>Evidence Based Healthcare , Journal Article</td>\n",
       "      <td>2020</td>\n",
       "      <td>Implementation and effectiveness of adolescent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.44597e+09</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>2020</td>\n",
       "      <td>The effects of age and trait anxiety on avoida...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StoreId                                  documentType  year  \\\n",
       "0  2.44597e+09                               Journal Article  2020   \n",
       "1  2.44597e+09                               Journal Article  2020   \n",
       "2  2.44597e+09                               Journal Article  2020   \n",
       "3  2.43522e+09   Evidence Based Healthcare , Journal Article  2020   \n",
       "4  2.44597e+09                               Journal Article  2020   \n",
       "\n",
       "                                                Text  \n",
       "0  Threat rapidly disrupts reward reversal learni...  \n",
       "1  Fear in the context of pain: Lessons learned f...  \n",
       "2  The effects of positive interpretation bias on...  \n",
       "3  Implementation and effectiveness of adolescent...  \n",
       "4  The effects of age and trait anxiety on avoida...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data=df.Text.values.tolist()\n",
    "# Remove new line characters\n",
    "data=[re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data=[re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['threat', 'rapidly', 'disrupts', 'reward', 'reversal', 'learningthreat', 'changes', 'cognition', 'and', 'facilitates', 'adaptive', 'coping', 'however', 'when', 'threat', 'becomes', 'overwhelming', 'it', 'may', 'be', 'deleterious', 'to', 'mental', 'health', 'especially', 'for', 'vulnerable', 'individuals', 'flexible', 'decision', 'making', 'was', 'probed', 'with', 'reward', 'reversal', 'task', 'to', 'investigate', 'how', 'well', 'healthy', 'participants', 'can', 'adapt', 'to', 'changes', 'in', 'reward', 'contingency', 'when', 'they', 'expect', 'adverse', 'events', 'electric', 'shocks', 'in', 'comparison', 'to', 'safe', 'control', 'condition', 'the', 'threat', 'of', 'shock', 'significantly', 'impaired', 'reward', 'reversal', 'learning', 'moreover', 'enhanced', 'self', 'reported', 'threat', 'ratings', 'and', 'elevated', 'skin', 'conductance', 'levels', 'support', 'the', 'successful', 'induction', 'of', 'stressful', 'and', 'aversive', 'apprehensions', 'the', 'findings', 'are', 'in', 'line', 'with', 'literature', 'showing', 'the', 'stress', 'induced', 'inhibition', 'of', 'goal', 'directed', 'behavior', 'at', 'the', 'advantage', 'of', 'reflexive', 'habitual', 'response', 'style', 'notably', 'reversal', 'learning', 'was', 'rapidly', 'restored', 'with', 'the', 'omission', 'of', 'threat', 'through', 'several', 'cycles', 'of', 'threat', 'and', 'safety', 'contexts', 'within', 'one', 'experimental', 'session', 'these', 'results', 'extend', 'the', 'literature', 'and', 'illuminate', 'the', 'immediate', 'consequence', 'of', 'sustained', 'threatening', 'stressor', 'and', 'its', 'removal', 'on', 'decision', 'making', 'better', 'knowledge', 'of', 'the', 'immediate', 'effects', 'of', 'anticipatory', 'anxiety', 'on', 'behavior', 'could', 'improve', 'understanding', 'of', 'psychopathology', 'and', 'may', 'be', 'informative', 'for', 'the', 'development', 'of', 'effective', 'therapy', 'for', 'anxiety', 'and', 'emotion', 'dysregulation']]\n"
     ]
    }
   ],
   "source": [
    "#Clean up text\n",
    "data_words=list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram=gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram=gensim.models.Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod=gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod=gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['threat', 'rapidly', 'disrupts', 'reward', 'reversal', 'learningthreat', 'changes', 'cognition', 'and', 'facilitates', 'adaptive', 'coping', 'however', 'when', 'threat', 'becomes', 'overwhelming', 'it', 'may', 'be', 'deleterious', 'to', 'mental', 'health', 'especially', 'for', 'vulnerable', 'individuals', 'flexible', 'decision_making', 'was', 'probed', 'with', 'reward', 'reversal', 'task', 'to', 'investigate', 'how', 'well', 'healthy', 'participants', 'can', 'adapt', 'to', 'changes', 'in', 'reward', 'contingency', 'when', 'they', 'expect', 'adverse_events', 'electric', 'shocks', 'in', 'comparison', 'to', 'safe', 'control', 'condition', 'the', 'threat', 'of', 'shock', 'significantly', 'impaired', 'reward', 'reversal', 'learning', 'moreover', 'enhanced', 'self', 'reported', 'threat', 'ratings', 'and', 'elevated', 'skin_conductance', 'levels', 'support', 'the', 'successful', 'induction', 'of', 'stressful', 'and', 'aversive', 'apprehensions', 'the', 'findings', 'are', 'in', 'line', 'with', 'literature', 'showing', 'the', 'stress', 'induced', 'inhibition', 'of', 'goal_directed', 'behavior', 'at', 'the', 'advantage', 'of', 'reflexive', 'habitual', 'response', 'style', 'notably', 'reversal', 'learning', 'was', 'rapidly', 'restored', 'with', 'the', 'omission', 'of', 'threat', 'through', 'several', 'cycles', 'of', 'threat', 'and', 'safety', 'contexts', 'within', 'one', 'experimental', 'session', 'these', 'results', 'extend', 'the', 'literature', 'and', 'illuminate', 'the', 'immediate', 'consequence', 'of', 'sustained', 'threatening', 'stressor', 'and', 'its', 'removal', 'on', 'decision_making', 'better', 'knowledge', 'of', 'the', 'immediate', 'effects', 'of', 'anticipatory', 'anxiety', 'on', 'behavior', 'could', 'improve', 'understanding', 'of', 'psychopathology', 'and', 'may', 'be', 'informative', 'for', 'the', 'development', 'of', 'effective', 'therapy', 'for', 'anxiety', 'and', 'emotion', 'dysregulation']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['threat', 'rapidly', 'disrupt', 'reward', 'reversal', 'learningthreat', 'change', 'cognition', 'facilitate', 'adaptive', 'cope', 'however', 'threat', 'become', 'overwhelming', 'deleterious', 'mental', 'health', 'especially', 'vulnerable', 'individual', 'flexible', 'decision_making', 'probe', 'reward', 'reversal', 'task', 'investigate', 'well', 'healthy', 'participant', 'adapt', 'change', 'reward', 'contingency', 'expect', 'adverse_event', 'electric', 'shock', 'comparison', 'safe', 'control', 'condition', 'threat', 'shock', 'significantly', 'impair', 'reward', 'reversal', 'learn', 'moreover', 'enhanced', 'self', 'report', 'threat', 'rating', 'elevate', 'level', 'support', 'successful', 'induction', 'stressful', 'aversive', 'apprehension', 'finding', 'line', 'literature', 'show', 'stress', 'induce', 'inhibition', 'goal_directe', 'behavior', 'advantage', 'reflexive', 'habitual', 'response', 'style', 'notably', 'reversal', 'learning', 'rapidly', 'restore', 'omission', 'threat', 'several', 'cycle', 'threat', 'safety', 'context', 'experimental', 'session', 'result', 'extend', 'literature', 'illuminate', 'immediate', 'consequence', 'sustain', 'threaten', 'stressor', 'removal', 'decision_make', 'well', 'knowledge', 'immediate', 'effect', 'anticipatory', 'anxiety', 'behavior', 'improve', 'understand', 'psychopathology', 'informative', 'development', 'effective', 'therapy', 'anxiety', 'emotion', 'dysregulation']]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# 2. Create Dictionary needed for topic modelling\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# 3. Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# 4. Term Document Frequency and Create a bag of words\n",
    "bow_corpus = bow(dictionary=id2word, processed_docs=texts)\n",
    "\n",
    "# 5. Calculate low_tfidf_words\n",
    "# Keep only words with tfidf ranking <= x * len(dictionary)\n",
    "x = 0.2\n",
    "total_word_count, DictDocFreq = tf_df(bow_corpus, id2word)\n",
    "sorted_TFIDF = sort_tfidf(bow_corpus, total_word_count, DictDocFreq)\n",
    "low_tfidf_words = get_low_tfidf_words(x, id2word, sorted_TFIDF)\n",
    "\n",
    "# 6. Filter out least frequently used words\n",
    "no_below = 0.01\n",
    "keep_n = 10000\n",
    "dict_least_freq_filtered = filter_least_frequent(id2word, texts, \n",
    "                                                 no_below, keep_n)\n",
    "\n",
    "# 7. Filter out most commonly used words (i.e. words with low TF-IDF score)\n",
    "dict_tfidf_filtered = filter_most_common(dict_least_freq_filtered, low_tfidf_words)\n",
    "\n",
    "# 8. Create the second bag of words - bow_corpus_TFIDFfiltered, \n",
    "# created after least frequently and most commonly used words were filtered out.\n",
    "corpus = bow(dict_tfidf_filtered, texts)\n",
    "\n",
    "# View\n",
    "[[(dict_tfidf_filtered[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dict_tfidf_filtered,\n",
    "                                           num_topics=50,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dict_tfidf_filtered, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df=pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row=row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row=sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0:  # => dominant topic\n",
    "                wp=ldamodel.show_topic(topic_num)\n",
    "                topic_keywords=\", \".join([word for word, prop in wp])\n",
    "                sent_topics_df=sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "     # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 50\n",
    "topic_words = []\n",
    "for i in range(num_topics):\n",
    "    tt = lda_model.get_topic_terms(i,10)\n",
    "    topic_words.append([id2word[pair[0]] for pair in tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"Journal2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(alpha='auto', corpus=corpus, num_topics=num_topics, \n",
    "                                                id2word=dictionary, random_state=5)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dict_tfidf_filtered, corpus=corpus, \n",
    "                                                        texts=data_lemmatized, start=2, limit=100, step=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show graph and compute the optimal number of topics\n",
    "\n",
    "limit=100; start=2; step=8;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "\n",
    "def differences(values, x):\n",
    "    \"\"\"\n",
    "    Returns (optimal number of topics, largest jump, coherance value after jump)\n",
    "    Params:\n",
    "        values: list of coherance numbers, \n",
    "        x: num topics\n",
    "    \"\"\"\n",
    "    x = list(x)\n",
    "    max_diff = (0,0,0)\n",
    "    for i in range(1,len(values)):\n",
    "        print(x[i],values[i],values[i]-values[i-1])\n",
    "        if values[i]-values[i-1] > max_diff[1]:\n",
    "            max_diff = (x[i], values[i]-values[i-1], values[i])\n",
    "    return max_diff\n",
    "\n",
    "differences(coherence_values, x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph and compute the optimal number of topics\n",
    "\n",
    "limit=100; start=2; step=8;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "\n",
    "def differences(values, x):\n",
    "    \"\"\"\n",
    "    Returns (optimal number of topics, largest jump, coherance value after jump)\n",
    "    Params:\n",
    "        values: list of coherance numbers, \n",
    "        x: num topics\n",
    "    \"\"\"\n",
    "    x = list(x)\n",
    "    max_diff = (0,0,0)\n",
    "    for i in range(1,len(values)):\n",
    "        print(x[i],values[i],values[i]-values[i-1])\n",
    "        if values[i]-values[i-1] > max_diff[1]:\n",
    "            max_diff = (x[i], values[i]-values[i-1], values[i])\n",
    "    return max_diff\n",
    "\n",
    "differences(coherence_values, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}